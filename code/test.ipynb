{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af263168-66d3-4388-9492-2bfd6aec9d6e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# DPI 1000\n",
    "from ContaminationExtractor import Paper_Contamination \n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "save_fullpath = \"keyword_100000.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList.txt\"\n",
    "\n",
    "\n",
    "\n",
    "Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18c505c-3b70-4a50-bb15-e6879e3c4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TotalFiles\": \"100\", \"ExecutionTimeInSeconds\": \"4,364\"}\n"
     ]
    }
   ],
   "source": [
    "# DPI 1000\n",
    "from ContaminationExtractor import Paper_Contamination \n",
    "\n",
    "\n",
    "save_fullpath = \"keyword_500000.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList.txt\"\n",
    "\n",
    "\n",
    "\n",
    "Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a8a1fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfile_names\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.DS_Store\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"
     ]
    }
   ],
   "source": [
    "for paper in file_names:\n",
    "    if paper == '.DS_Store':\n",
    "        continue\n",
    "    print(f\"üßæ OCR ÏãúÏûë: {paper}\")\n",
    "    paper_name = folder_path + '/' + paper\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "        print(f\"‚úÖ OCR ÏÑ±Í≥µ: {paper} ‚Üí Ï¥ù ÌéòÏù¥ÏßÄ Ïàò {len(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OCR Ïã§Ìå®: {paper} ‚Üí {e}\")\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cf31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from paper_ocr import paper_ocr\n",
    "    import kss\n",
    "    import re\n",
    "    import os\n",
    "    import warnings\n",
    "    import time\n",
    "    import json\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import difflib\n",
    "    from tqdm import tqdm\n",
    "    from difflib import SequenceMatcher\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    # Í≤ΩÍ≥† Î¨¥Ïãú\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Sentence DB\n",
    "    historical_data = pd.read_excel(\n",
    "        historical_data_fullpath, engine=\"openpyxl\")\n",
    "    queries = historical_data.values.tolist()\n",
    "\n",
    "    # Í≤©Ïã§ Î∞è Í∏∞Í∏∞ DB\n",
    "    DB_room_df = pd.read_excel(DB_fullpath, sheet_name='Í≤©Ïã§ DB')\n",
    "    DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='Í∏∞Í∏∞ DB')\n",
    "\n",
    "    # Ìè¥Îçî ÎÇ¥Ïùò ÌååÏùº Ïù¥Î¶Ñ Í∞ÄÏ†∏Ïò§Í∏∞ (Ï≤´ Îëê Í∞úÏùò ÌååÏùºÎßå Ï≤òÎ¶¨)\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "            file_names.append(file_name)\n",
    "    file_names = file_names[:2]  # Ï≤´ Îëê Í∞ú ÌååÏùºÎßå Ï≤òÎ¶¨\n",
    "\n",
    "    # Mistral Î™®Îç∏ Î°úÎìú\n",
    "    mistral_model_path = \"/home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "    llm = Llama(model_path=mistral_model_path, n_ctx=2048,\n",
    "                n_threads=8, n_gpu_layers=30, verbose=False)\n",
    "\n",
    "    result_line_list = []\n",
    "    errorfile = []\n",
    "\n",
    "    for paper in file_names:\n",
    "        if paper == '.DS_Store':\n",
    "            continue\n",
    "        paper_name = folder_path + '/' + paper\n",
    "\n",
    "        # OCR\n",
    "        try:\n",
    "            data = paper_ocr(paper_name)\n",
    "        except:\n",
    "            errorfile.append(paper_name)\n",
    "            continue\n",
    "\n",
    "        # Í≥µÎ∞± Ï†úÍ±∞\n",
    "        newdata = []\n",
    "        for page in data:\n",
    "            preprocessed_page = [\n",
    "                re.sub(' +', ' ', sentence).strip() for sentence in page]\n",
    "            newdata.append(preprocessed_page)\n",
    "\n",
    "        # Î¨∏Ïû• Î∂ÑÎ¶¨\n",
    "        sentence_temp = []\n",
    "        if paper_name[-3:].lower() == 'txt' or paper_name[-3:].lower() == 'pdf':\n",
    "            for j_idx, j in enumerate(newdata):\n",
    "                sentences = []\n",
    "                for item in j:\n",
    "                    sentences.extend(kss.split_sentences(item))\n",
    "                for sentence_idx, sentence in enumerate(sentences):\n",
    "                    sentence_temp.append(\n",
    "                        {'Î¨∏Ïû•': sentence, 'ÌéòÏù¥ÏßÄ': j_idx + 1, 'Î¨∏Ïû•ÏúÑÏπò': sentence_idx + 1})\n",
    "\n",
    "        # 10Ïûê Ïù¥Ìïò Î¨∏Ïû• Ï†úÍ±∞\n",
    "        min_length = 10\n",
    "        sentence_temp = [s for s in sentence_temp if len(s['Î¨∏Ïû•']) > min_length]\n",
    "        sentence = pd.DataFrame(sentence_temp)\n",
    "        sentence_list = [item['Î¨∏Ïû•'] for item in sentence_temp]\n",
    "\n",
    "        # Í≤∞Í≥º ÎπÑÍµê\n",
    "        result_df_list = []\n",
    "        similarity_cache = {}\n",
    "\n",
    "        for query_row in queries:\n",
    "            query = query_row[0]\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "                futures = []\n",
    "\n",
    "                for target in sentence_list:\n",
    "                    key = (query, target)\n",
    "                    if key in similarity_cache:\n",
    "                        futures.append(executor.submit(\n",
    "                            lambda q=query, t=target: (q, t, similarity_cache[key])))\n",
    "                    else:\n",
    "                        simple_similarity = SequenceMatcher(\n",
    "                            None, query, target).ratio()\n",
    "                        if simple_similarity < 0.2:\n",
    "                            continue\n",
    "                        futures.append(executor.submit(\n",
    "                            lambda q=query, t=target: (\n",
    "                                q, t,\n",
    "                                float(llm(f\"\"\"Îã§Ïùå Îëê Î¨∏Ïû•ÏùÄ ÏñºÎßàÎÇò Ïú†ÏÇ¨Ìï©ÎãàÍπå? 0Î∂ÄÌÑ∞ 1 ÏÇ¨Ïù¥Ïùò Ï†êÏàòÎ°ú Ï†ïÏàòÎÇò ÏÜåÏàòÎ°úÎßå ÎãµÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "Î¨∏Ïû•1: {q}\n",
    "Î¨∏Ïû•2: {t}\n",
    "Ï†êÏàò:\"\"\", max_tokens=10, stop=[\"\\n\"])['choices'][0]['text'].strip())\n",
    "                            )\n",
    "                        ))\n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            q, t, score = future.result()\n",
    "                            similarity_cache[(q, t)] = score\n",
    "                            if score > 0.3:\n",
    "                                result_df_list.append(pd.DataFrame({\n",
    "                                    'Query': [q],\n",
    "                                    'Ïò§ÏóºÎ¨∏Ïû•': [t.strip()],\n",
    "                                    'score': [score]\n",
    "                                }))\n",
    "                        except:\n",
    "                            continue\n",
    "                        if len(result_df_list) == 0:\n",
    "                            continue\n",
    "\n",
    "        # Í≤∞Í≥º DataFrame ÏÉùÏÑ±\n",
    "        if result_df_list:\n",
    "            result = pd.concat(result_df_list, ignore_index=True)\n",
    "            result = result.sort_values(by='score', ascending=False)\n",
    "            result_fin = result.drop_duplicates(['Ïò§ÏóºÎ¨∏Ïû•'], keep='first')\n",
    "            result_similarity = result_fin.reset_index(drop=True)\n",
    "\n",
    "            # ÌéòÏù¥ÏßÄ, Î¨∏Ïû•ÏúÑÏπò Ï∂îÍ∞Ä\n",
    "            result_similarity['ÌéòÏù¥ÏßÄ'] = None\n",
    "            result_similarity['Î¨∏Ïû•ÏúÑÏπò'] = None\n",
    "\n",
    "            for index, row in result_similarity.iterrows():\n",
    "                query = row['Ïò§ÏóºÎ¨∏Ïû•']\n",
    "                match_row = sentence[sentence['Î¨∏Ïû•'] == query]\n",
    "                if not match_row.empty:\n",
    "                    result_similarity.at[index, 'ÌéòÏù¥ÏßÄ'] = match_row['ÌéòÏù¥ÏßÄ'].values[0]\n",
    "                    result_similarity.at[index,\n",
    "                                         'Î¨∏Ïû•ÏúÑÏπò'] = match_row['Î¨∏Ïû•ÏúÑÏπò'].values[0]\n",
    "\n",
    "            # Í≤∞Í≥º Ï∂úÎ†•\n",
    "            print(f\"Paper: {paper}\")\n",
    "            print(result_similarity[['Query', 'Ïò§ÏóºÎ¨∏Ïû•', 'score']].head())  # Ï≤´ 5Í∞ú Í≤∞Í≥º Ï∂úÎ†•\n",
    "\n",
    "    # Í≤∞Í≥ºÎ•º CSVÎ°ú Ï†ÄÏû•\n",
    "    result_line = pd.DataFrame(result_line_list)\n",
    "    try:\n",
    "        result_line = result_line.sort_values(by='score', ascending=False)\n",
    "    except:\n",
    "        pass\n",
    "    result_line = result_line.drop_duplicates(['Ïò§ÏóºÎ¨∏Ïû•', 'Ïû•ÏÜåÌëúÌòÑ'], keep='first')\n",
    "    try:\n",
    "        result_line = result_line.drop(columns=['score'])\n",
    "    except:\n",
    "        pass\n",
    "    result_line = result_line.reset_index(drop=True)\n",
    "\n",
    "    if result_line.empty:\n",
    "        result_line['ÌååÏùºÏù¥Î¶Ñ'] = None\n",
    "        result_line['Ïò§ÏóºÎ¨∏Ïû•'] = None\n",
    "        result_line['ÌéòÏù¥ÏßÄ'] = None\n",
    "        result_line['Î¨∏Ïû•ÏúÑÏπò'] = None\n",
    "        result_line['Ïû•ÏÜåÎ™Ö'] = None\n",
    "        result_line['Í∏∞Í∏∞Î™Ö'] = None\n",
    "        result_line['Ïû•ÏÜåÌëúÌòÑ'] = None\n",
    "        result_line['Í∏∞Í∏∞ÌëúÌòÑ'] = None\n",
    "\n",
    "    result_line.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Ïò§Î•ò ÌååÏùº Í∏∞Î°ù\n",
    "    with open(errorfile_fullpath, 'w') as file:\n",
    "        for item in errorfile:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "    # Ïã§Ìñâ Ï¢ÖÎ£å ÏãúÍ∞Ñ Í∏∞Î°ù\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Ïã§Ìñâ ÏãúÍ∞Ñ Í≥ÑÏÇ∞\n",
    "    execution_time = end_time - start_time\n",
    "    total_files = len(file_names)\n",
    "\n",
    "    # JSON ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•\n",
    "    JSON_text = {\n",
    "        \"TotalFiles\": f\"{total_files:,}\",\n",
    "        \"ExecutionTimeInSeconds\": f\"{execution_time:,.0f}\"\n",
    "    }\n",
    "\n",
    "    json_data = json.dumps(JSON_text)\n",
    "    print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a544dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ÌÖåÏä§Ìä∏ ÏòàÏãú (Í≤ΩÎ°ú ÏàòÏ†ïÌï¥ÏÑú ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpaper_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/taco/Documents/projects/jung/model/paper_folder/[Í≥†Î¶¨4Ìò∏Í∏∞]Ï¶ùÍ∏∞Î∞úÏÉùÍ∏∞ \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m ÏàòÏã§ Î∞∞ÏàòÍ¥ÄÏùò Î∞∞ÏàòÎ∞∏Î∏å Ïö©Ï†ëÎ∂Ä ÎàÑÏÑ§Î∂ÄÏúÑ Ï†ïÎπÑÎ•º ÏúÑÌïú ÏõêÏûêÎ°ú ÏàòÎèôÏ†ïÏßÄ.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Í≤∞Í≥º ÌôïÏù∏\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mpaper_ocr\u001b[0;34m(paper_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[1;32m     35\u001b[0m         image_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(page)\n\u001b[0;32m---> 36\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         paper_temp\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paper_temp\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[0;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[1;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_cv_grey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizontal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocklist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[0;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[1;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[0;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mignore_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/recognition.py:206\u001b[0m, in \u001b[0;36mget_text\u001b[0;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    202\u001b[0m     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_normal, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# predict first round\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mignore_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_group_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# predict second round\u001b[39;00m\n\u001b[1;32m    210\u001b[0m low_confident_idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result1) \u001b[38;5;28;01mif\u001b[39;00m (item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m contrast_ths)]\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/recognition.py:111\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[0;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m length_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([batch_max_length] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    109\u001b[0m text_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size, batch_max_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 111\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_for_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Select max probabilty (greedy decoding) then decode index to character\u001b[39;00m\n\u001b[1;32m    114\u001b[0m preds_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m batch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/model/vgg_model.py:25\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input, text)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, text):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Feature extraction stage \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatureExtraction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool(visual_feature\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     27\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m visual_feature\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/model/modules.py:124\u001b[0m, in \u001b[0;36mVGG_FeatureExtractor.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import traceback\n",
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/taco/Documents/projects/jung/poppler_2301_build/lib\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    try:\n",
    "        # GPU ÏÇ¨Ïö© ÏÑ§Ï†ï, Î™®Îç∏ Í≤ΩÎ°ú Î™ÖÏãú\n",
    "        reader = easyocr.Reader(\n",
    "            ['en', 'ko'],\n",
    "            gpu=False,\n",
    "            model_storage_directory='model',\n",
    "            user_network_directory='model',\n",
    "            download_enabled=False\n",
    "        )\n",
    "\n",
    "        # .txt ÌååÏùº Ï≤òÎ¶¨\n",
    "        if paper_name[-3:].lower() == 'txt':\n",
    "            with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "                paper_temp = f.read()\n",
    "                paper_temp = [[paper_temp]]\n",
    "\n",
    "        # .pdf ÌååÏùº Ï≤òÎ¶¨\n",
    "        elif paper_name[-3:].lower() == 'pdf':\n",
    "            pages = convert_from_path(\n",
    "                paper_name,\n",
    "                dpi=800,\n",
    "                poppler_path=\"/home/taco/Documents/projects/jung/poppler_2301_build/bin\"  # Î¶¨ÎàÖÏä§ poppler Í≤ΩÎ°ú (ÌôòÍ≤ΩÏóê Îî∞Îùº Ï°∞Ï†ï Í∞ÄÎä•)\n",
    "            )\n",
    "            paper_temp = []\n",
    "            for page in pages:\n",
    "                image_array = np.array(page)\n",
    "                result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "                paper_temp.append(result)\n",
    "\n",
    "        return paper_temp\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Î≥ÄÌôò Ïã§Ìå®!\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏòàÏãú (Í≤ΩÎ°ú ÏàòÏ†ïÌï¥ÏÑú ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî)\n",
    "output = paper_ocr(\"/home/taco/Documents/projects/jung/model/paper_folder/[Í≥†Î¶¨4Ìò∏Í∏∞]Ï¶ùÍ∏∞Î∞úÏÉùÍ∏∞ 'A' ÏàòÏã§ Î∞∞ÏàòÍ¥ÄÏùò Î∞∞ÏàòÎ∞∏Î∏å Ïö©Ï†ëÎ∂Ä ÎàÑÏÑ§Î∂ÄÏúÑ Ï†ïÎπÑÎ•º ÏúÑÌïú ÏõêÏûêÎ°ú ÏàòÎèôÏ†ïÏßÄ.pdf\")\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "if output is not None:\n",
    "    for i, page in enumerate(output):\n",
    "        print(f\"\\n--- Page {i+1} ---\")\n",
    "        print(\"\\n\".join(page))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77204ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3080 Laptop GPU) - 7619 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =   181.04 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 356 (with bs=512), 1 (with bs=1)\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ ÏÑ†ÌÉùÎêú ÌååÏùº: paper_folder/Ï£ºÏ¶ùÍ∏∞Î™®Í¥Ä Î∞∞Í∏∞Î∞∏Î∏å Ï†ïÎπÑ, Ï†ÄÏïïÌÑ∞Îπà ÎÇ†Í∞ú Ï†êÍ≤Ä, MSR Expansion Bellows Joint Ï†êÍ≤Ä Îì±ÏùÑ ÏúÑÌïú ÏõêÏûêÎ°ú Ïàò....pdf\n",
      "üîÑ LLM Î™®Îç∏ Î°úÎìú Ï§ë...\n",
      "‚úÖ LLM Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "\n",
      "üì• Sentence DB Î°úÎî© Ï§ë...\n",
      "‚úÖ Î¨∏Ïû• ÏøºÎ¶¨ Ïàò: 136\n",
      "üîç OCR ÏãúÏûë...\n",
      "‚úÖ OCR ÏÑ±Í≥µ\n",
      "‚úÖ OCR ÌõÑ ÌéòÏù¥ÏßÄ Ïàò: 1\n",
      "‚úÖ Î∂ÑÎ¶¨Îêú Î¨∏Ïû• Ïàò: 19\n",
      "‚úÖ 10Ïûê Ïù¥ÏÉÅ Î¨∏Ïû• Ïàò: 5\n",
      "\n",
      "üî¨ LLM Ïú†ÏÇ¨ÎèÑ ÌÖåÏä§Ìä∏\n",
      "üî∏ Query Î¨∏Ïû•:\n",
      "ÏõêÏûêÎ°úÍ±¥Î¨º 2Ï∏µÏùò ÎÉâÍ∞ÅÏàò Ï†ÄÏû•ÌÉ±ÌÅ¨ÏóêÏÑú ÎàÑÏÑ§Ïù¥ Î∞úÏÉùÌïòÏó¨ Î∞©ÏÇ¨Îä• Í∞êÏßÄÍ∏∞Í∞Ä ÏûëÎèôÌïòÏòÄÎã§. ÏûëÏóÖÏûê BÍ∞Ä ÌÉ±ÌÅ¨Ïùò Î∞∏Î∏åÎ•º ÏûòÎ™ª Ï°∞ÏûëÌïú Í≤ÉÏù¥ ÏõêÏù∏ÏúºÎ°ú ÌååÏïÖÎêòÏóàÎã§.\n",
      "üîπ Target Î¨∏Ïû•:\n",
      "ÏÇ¨Í±¥Î∞úÏÉùÏùºÏûê 85/10/19 23:58\n",
      "üîÅ SequenceMatcher Ïú†ÏÇ¨ÎèÑ: 0.058\n",
      "‚õî Ïú†ÏÇ¨ÎèÑ ÎÇÆÏïÑÏÑú Ïä§ÌÇµ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import kss\n",
    "import traceback\n",
    "from llama_cpp import Llama\n",
    "from difflib import SequenceMatcher\n",
    "from paper_ocr import paper_ocr  # OCR Ìï®Ïàò ÏÇ¨Ïö© Í∞ÄÎä• ÏÉÅÌÉúÏó¨Ïïº Ìï®\n",
    "\n",
    "# üîß ÏÑ§Ï†ï\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "mistral_model_path = \"/home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# Ï†êÏàò Ï∂îÏ∂ú Ìï®Ïàò\n",
    "def extract_score(text):\n",
    "    try:\n",
    "        match = re.search(r\"([0-9]+\\.?[0-9]*)\", text)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Step 0. Ï≤´ ÌååÏùºÎßå ÏÑ†ÌÉù\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.pdf') and not f.startswith('.')]\n",
    "if not file_names:\n",
    "    raise FileNotFoundError(\"‚ùå PDF ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "paper = file_names[0]\n",
    "paper_path = os.path.join(folder_path, paper)\n",
    "print(f\"\\nüìÑ ÏÑ†ÌÉùÎêú ÌååÏùº: {paper_path}\")\n",
    "\n",
    "# Step 1. LLM Î™®Îç∏ Î°úÎìú\n",
    "print(\"üîÑ LLM Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
    "llm = Llama(model_path=mistral_model_path, n_ctx=512, n_threads=4, n_gpu_layers=0, verbose=True)\n",
    "print(\"‚úÖ LLM Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\\n\")\n",
    "\n",
    "# Step 2. Sentence DB\n",
    "print(\"üì• Sentence DB Î°úÎî© Ï§ë...\")\n",
    "queries = pd.read_excel(historical_data_fullpath, engine=\"openpyxl\").values.tolist()\n",
    "print(f\"‚úÖ Î¨∏Ïû• ÏøºÎ¶¨ Ïàò: {len(queries)}\")\n",
    "\n",
    "# Step 3. OCR\n",
    "try:\n",
    "    print(\"üîç OCR ÏãúÏûë...\")\n",
    "    data = paper_ocr(paper_path)\n",
    "    print(\"‚úÖ OCR ÏÑ±Í≥µ\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå OCR Ïã§Ìå®\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# Step 4. Í≥µÎ∞± Ï†úÍ±∞\n",
    "newdata = []\n",
    "for page in data:\n",
    "    preprocessed = [re.sub(' +', ' ', s).strip() for s in page]\n",
    "    newdata.append(preprocessed)\n",
    "print(f\"‚úÖ OCR ÌõÑ ÌéòÏù¥ÏßÄ Ïàò: {len(newdata)}\")\n",
    "\n",
    "# Step 5. Î¨∏Ïû• Î∂ÑÎ¶¨\n",
    "sentence_temp = []\n",
    "for j_idx, page in enumerate(newdata):\n",
    "    sentences = []\n",
    "    for item in page:\n",
    "        sentences.extend(kss.split_sentences(item))\n",
    "    for sentence_idx, sentence in enumerate(sentences):\n",
    "        sentence_temp.append({'Î¨∏Ïû•': sentence, 'ÌéòÏù¥ÏßÄ': j_idx+1, 'Î¨∏Ïû•ÏúÑÏπò': sentence_idx+1})\n",
    "print(f\"‚úÖ Î∂ÑÎ¶¨Îêú Î¨∏Ïû• Ïàò: {len(sentence_temp)}\")\n",
    "\n",
    "# Step 6. 10Ïûê Ïù¥ÏÉÅ ÌïÑÌÑ∞\n",
    "sentence_temp = [s for s in sentence_temp if len(s['Î¨∏Ïû•']) > 10]\n",
    "sentence_list = [s['Î¨∏Ïû•'] for s in sentence_temp]\n",
    "print(f\"‚úÖ 10Ïûê Ïù¥ÏÉÅ Î¨∏Ïû• Ïàò: {len(sentence_list)}\")\n",
    "\n",
    "# Step 7. LLM Ïú†ÏÇ¨ÎèÑ ÎπÑÍµê (1Í∞úÎßå)\n",
    "print(\"\\nüî¨ LLM Ïú†ÏÇ¨ÎèÑ ÌÖåÏä§Ìä∏\")\n",
    "query = queries[0][0].strip()\n",
    "target = sentence_list[0].strip()\n",
    "\n",
    "print(f\"üî∏ Query Î¨∏Ïû•:\\n{query}\")\n",
    "print(f\"üîπ Target Î¨∏Ïû•:\\n{target}\")\n",
    "\n",
    "sim_score = SequenceMatcher(None, query, target).ratio()\n",
    "print(f\"üîÅ SequenceMatcher Ïú†ÏÇ¨ÎèÑ: {sim_score:.3f}\")\n",
    "if sim_score < 0.3:\n",
    "    print(\"‚õî Ïú†ÏÇ¨ÎèÑ ÎÇÆÏïÑÏÑú Ïä§ÌÇµ\")\n",
    "else:\n",
    "    prompt = f\"\"\"Îëê Î¨∏Ïû•Ïùò Ïú†ÏÇ¨ÎèÑÎ•º 0ÏóêÏÑú 1 ÏÇ¨Ïù¥ ÏÜåÏàòÏ†ê Ï†êÏàòÎ°úÎßå Ïà´Ïûê ÌïòÎÇòÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. Îã§Î•∏ ÎßêÏùÄ ÌïòÏßÄ ÎßàÏÑ∏Ïöî.\\nÎ¨∏Ïû•1: {query}\\nÎ¨∏Ïû•2: {target}\\nÎãµ:\"\"\"\n",
    "    try:\n",
    "        output = llm(prompt, max_tokens=10, stop=[\"\\n\"])\n",
    "        score = extract_score(output['choices'][0]['text'])\n",
    "        print(f\"‚úÖ LLM Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {score}\")\n",
    "    except:\n",
    "        print(\"‚ùå LLM Ìò∏Ï∂ú Ïã§Ìå®\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c98c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import difflib\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    # PDF ‚Üí Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò\n",
    "    pages = convert_from_path(pdf_path, dpi=800, poppler_path=poppler_path)\n",
    "\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    for gpu_mode in [False, True]:\n",
    "        reader = easyocr.Reader(['en', 'ko'], gpu=gpu_mode,\n",
    "                                model_storage_directory='model',\n",
    "                                user_network_directory='model',\n",
    "                                download_enabled=False)\n",
    "        key = \"gpu\" if gpu_mode else \"cpu\"\n",
    "\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            text = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[key].append(\"\\n\".join(text))\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_output, gpu_output):\n",
    "    print(\"=== üîç Difference between CPU and GPU OCR ===\\n\")\n",
    "    for page_idx, (cpu_text, gpu_text) in enumerate(zip(cpu_output, gpu_output)):\n",
    "        print(f\"\\n--- Page {page_idx + 1} ---\")\n",
    "        cpu_lines = cpu_text.splitlines()\n",
    "        gpu_lines = gpu_text.splitlines()\n",
    "        diff = difflib.unified_diff(cpu_lines, gpu_lines, fromfile='CPU', tofile='GPU', lineterm='')\n",
    "        for line in diff:\n",
    "            print(line)\n",
    "\n",
    "# ÏÇ¨Ïö© ÏòàÏãú\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[Í≥†Î¶¨4Ìò∏Í∏∞]Ï¶ùÍ∏∞Î∞úÏÉùÍ∏∞ 'A' ÏàòÏã§ Î∞∞ÏàòÍ¥ÄÏùò Î∞∞ÏàòÎ∞∏Î∏å Ïö©Ï†ëÎ∂Ä ÎàÑÏÑ§Î∂ÄÏúÑ Ï†ïÎπÑÎ•º ÏúÑÌïú ÏõêÏûêÎ°ú ÏàòÎèôÏ†ïÏßÄ.pdf\"\n",
    "\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from ContaminationExtractor import Paper_Contamination  # Ïô∏Î∂Ä Î™®Îìà Î∂àÎü¨Ïò§Í∏∞\n",
    "\n",
    "def Paper_Contamination_Compare(gpu_result_path, cpu_result_path):\n",
    "    print(\"\\nüìä [Í≤∞Í≥º ÎπÑÍµê Ï§ë]\")\n",
    "\n",
    "    gpu_df = pd.read_csv(gpu_result_path)\n",
    "    cpu_df = pd.read_csv(cpu_result_path)\n",
    "\n",
    "    gpu_sentences = set(gpu_df['Ïò§ÏóºÎ¨∏Ïû•'].dropna().str.strip().unique())\n",
    "    cpu_sentences = set(cpu_df['Ïò§ÏóºÎ¨∏Ïû•'].dropna().str.strip().unique())\n",
    "\n",
    "    print(f\"Ï¥ù Î¨∏Ïû• Ïàò (GPU): {len(gpu_sentences)}\")\n",
    "    print(f\"Ï¥ù Î¨∏Ïû• Ïàò (CPU): {len(cpu_sentences)}\")\n",
    "\n",
    "    gpu_only = gpu_sentences - cpu_sentences\n",
    "    cpu_only = cpu_sentences - gpu_sentences\n",
    "\n",
    "    print(f\"\\nüîç GPU/CPU Í≤∞Í≥ºÏóêÏÑú ÏÑúÎ°ú Îã§Î•∏ Ïò§ÏóºÎ¨∏Ïû• Ïàò: {len(gpu_only) + len(cpu_only)}\")\n",
    "    if not gpu_only and not cpu_only:\n",
    "        print(\"‚úÖ GPUÏôÄ CPU Í≤∞Í≥ºÎäî **ÏôÑÏ†ÑÌûà ÎèôÏùºÌï©ÎãàÎã§.**\")\n",
    "    else:\n",
    "        print(\"‚ùó Ï∞®Ïù¥ Î∞úÏÉù!\")\n",
    "        if gpu_only:\n",
    "            print(\"\\nüî∫ GPUÏóêÎßå ÏûàÎäî Î¨∏Ïû• (ÏòàÏãú ÏµúÎåÄ 5Í∞ú):\")\n",
    "            for sent in list(gpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "        if cpu_only:\n",
    "            print(\"\\nüîª CPUÏóêÎßå ÏûàÎäî Î¨∏Ïû• (ÏòàÏãú ÏµúÎåÄ 5Í∞ú):\")\n",
    "            for sent in list(cpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "\n",
    "\n",
    "def run_contamination_dual():\n",
    "    # Í≤ΩÎ°ú ÏÑ§Ï†ï (ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Î∞òÏòÅ)\n",
    "    folder_path = \"paper_folder\"\n",
    "    historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "    DB_fullpath = \"Model_DB.xlsx\"\n",
    "\n",
    "    # GPU Í≤ΩÎ°ú\n",
    "    gpu_result = \"keyword_100000_gpu.csv\"\n",
    "    gpu_error = \"ErrorFileList_gpu.txt\"\n",
    "\n",
    "    # CPU Í≤ΩÎ°ú\n",
    "    cpu_result = \"keyword_100000_cpu.csv\"\n",
    "    cpu_error = \"ErrorFileList_cpu.txt\"\n",
    "\n",
    "    # 1. GPU Ïã§Ìñâ\n",
    "    print(\"‚ñ∂ GPU Ïã§Ìñâ Ï§ë...\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPU ÏßÄÏ†ï\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=gpu_result,\n",
    "        folder_path=folder_path,\n",
    "        historical_data_fullpath=historical_data_fullpath,\n",
    "        DB_fullpath=DB_fullpath,\n",
    "        errorfile_fullpath=gpu_error\n",
    "    )\n",
    "\n",
    "    # 2. CPU Ïã§Ìñâ\n",
    "    print(\"\\n‚ñ∂ CPU Ïã§Ìñâ Ï§ë...\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # CPU Í∞ïÏ†ú ÏßÄÏ†ï\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=cpu_result,\n",
    "        folder_path=folder_path,\n",
    "        historical_data_fullpath=historical_data_fullpath,\n",
    "        DB_fullpath=DB_fullpath,\n",
    "        errorfile_fullpath=cpu_error\n",
    "    )\n",
    "\n",
    "    # 3. Í≤∞Í≥º ÎπÑÍµê\n",
    "    Paper_Contamination_Compare(gpu_result, cpu_result)\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "run_contamination_dual()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551afd-b1af-4eb5-93b5-0b1e2fce4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import torch\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    # PDF -> Ïù¥ÎØ∏ÏßÄ\n",
    "    pages = convert_from_path(pdf_path, dpi=400, poppler_path=poppler_path)\n",
    "\n",
    "    # Î¶¨Îçî ÏÉùÏÑ±\n",
    "    reader_cpu = easyocr.Reader(['en', 'ko'], gpu=False, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    reader_gpu = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        image_array = np.array(page)\n",
    "\n",
    "        # CPU OCR\n",
    "        try:\n",
    "            text_cpu = reader_cpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"cpu\"].append(\"\\n\".join(text_cpu))\n",
    "        except Exception as e:\n",
    "            results[\"cpu\"].append(f\"[CPU OCR Ïã§Ìå®: {str(e)}]\")\n",
    "\n",
    "        # GPU OCR\n",
    "        try:\n",
    "            text_gpu = reader_gpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"gpu\"].append(\"\\n\".join(text_gpu))\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                results[\"gpu\"].append(\"[GPU OCR Ïã§Ìå® - OOM]\")\n",
    "            else:\n",
    "                results[\"gpu\"].append(f\"[GPU OCR Ïã§Ìå®: {str(e)}]\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()  # GPU Ï∫êÏãú ÎπÑÏö∞Í∏∞\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_result, gpu_result):\n",
    "    for i, (cpu, gpu) in enumerate(zip(cpu_result, gpu_result)):\n",
    "        print(f\"\\nüìù Page {i+1}\")\n",
    "        print(\"üîµ CPU:\")\n",
    "        print(cpu)\n",
    "        print(\"üü¢ GPU:\")\n",
    "        print(gpu)\n",
    "\n",
    "# üîΩ Ïó¨Í∏∞Ïóê PDF Í≤ΩÎ°úÎßå ÎÑ£ÏúºÎ©¥ Ïã§ÌñâÎê®\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[Í≥†Î¶¨4Ìò∏Í∏∞]Ï¶ùÍ∏∞Î∞úÏÉùÍ∏∞ 'A' ÏàòÏã§ Î∞∞ÏàòÍ¥ÄÏùò Î∞∞ÏàòÎ∞∏Î∏å Ïö©Ï†ëÎ∂Ä ÎàÑÏÑ§Î∂ÄÏúÑ Ï†ïÎπÑÎ•º ÏúÑÌïú ÏõêÏûêÎ°ú ÏàòÎèôÏ†ïÏßÄ.pdf\"\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69070370-c0bc-4cb7-ae78-e19c8c2e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from contamination import Paper_Contamination  # Í∏∞Ï°¥ Ìï®Ïàò Î∂àÎü¨Ïò®Îã§Í≥† Í∞ÄÏ†ï\n",
    "\n",
    "def Paper_Contamination_Compare(gpu_result_path, cpu_result_path):\n",
    "    print(\"\\nüìä [Í≤∞Í≥º ÎπÑÍµê Ï§ë]\")\n",
    "\n",
    "    gpu_df = pd.read_csv(gpu_result_path)\n",
    "    cpu_df = pd.read_csv(cpu_result_path)\n",
    "\n",
    "    gpu_sentences = set(gpu_df['Ïò§ÏóºÎ¨∏Ïû•'].dropna().str.strip().unique())\n",
    "    cpu_sentences = set(cpu_df['Ïò§ÏóºÎ¨∏Ïû•'].dropna().str.strip().unique())\n",
    "\n",
    "    print(f\"Ï¥ù Î¨∏Ïû• Ïàò (GPU): {len(gpu_sentences)}\")\n",
    "    print(f\"Ï¥ù Î¨∏Ïû• Ïàò (CPU): {len(cpu_sentences)}\")\n",
    "\n",
    "    gpu_only = gpu_sentences - cpu_sentences\n",
    "    cpu_only = cpu_sentences - gpu_sentences\n",
    "\n",
    "    print(f\"\\nüîç GPU/CPU Í≤∞Í≥ºÏóêÏÑú ÏÑúÎ°ú Îã§Î•∏ Ïò§ÏóºÎ¨∏Ïû• Ïàò: {len(gpu_only) + len(cpu_only)}\")\n",
    "    if not gpu_only and not cpu_only:\n",
    "        print(\"‚úÖ GPUÏôÄ CPU Í≤∞Í≥ºÎäî **ÏôÑÏ†ÑÌûà ÎèôÏùºÌï©ÎãàÎã§.**\")\n",
    "    else:\n",
    "        print(\"‚ùó Ï∞®Ïù¥ Î∞úÏÉù!\")\n",
    "        if gpu_only:\n",
    "            print(\"\\nüî∫ GPUÏóêÎßå ÏûàÎäî Î¨∏Ïû• (ÏòàÏãú ÏµúÎåÄ 5Í∞ú):\")\n",
    "            for sent in list(gpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "        if cpu_only:\n",
    "            print(\"\\nüîª CPUÏóêÎßå ÏûàÎäî Î¨∏Ïû• (ÏòàÏãú ÏµúÎåÄ 5Í∞ú):\")\n",
    "            for sent in list(cpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "\n",
    "\n",
    "def run_contamination_dual():\n",
    "    # Í≥µÌÜµ ÏÑ§Ï†ï\n",
    "    paper_folder = \"/home/taco/Documents/projects/jung/model/paper_folder\"\n",
    "    historical_data = \"Historical_data.xlsx\"\n",
    "    DB_path = \"Model_DB.xlsx\"\n",
    "    result_dir = \"/home/taco/Documents/projects/jung/model/results\"\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # GPU Ïã§Ìñâ\n",
    "    print(\"‚ñ∂ GPU Ïã§Ìñâ Ï§ë...\")\n",
    "    gpu_result = os.path.join(result_dir, \"gpu_result.csv\")\n",
    "    gpu_error = os.path.join(result_dir, \"gpu_error.txt\")\n",
    "    torch.cuda.set_device(0) if torch.cuda.is_available() else None\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=gpu_result,\n",
    "        folder_path=paper_folder,\n",
    "        historical_data_fullpath=historical_data,\n",
    "        DB_fullpath=DB_path,\n",
    "        errorfile_fullpath=gpu_error\n",
    "    )\n",
    "\n",
    "    # CPU Ïã§Ìñâ\n",
    "    print(\"\\n‚ñ∂ CPU Ïã§Ìñâ Ï§ë...\")\n",
    "    cpu_result = os.path.join(result_dir, \"cpu_result.csv\")\n",
    "    cpu_error = os.path.join(result_dir, \"cpu_error.txt\")\n",
    "    torch.cuda.set_device(-1)  # CPU Í∞ïÏ†ú ÏßÄÏ†ï (optional, Ïã§Ï†ú Î™®Îç∏ ÎÇ¥ÏóêÏÑú device Í≥†Ï†ïÌï¥Ïïº ÏùòÎØ∏ ÏûàÏùå)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=cpu_result,\n",
    "        folder_path=paper_folder,\n",
    "        historical_data_fullpath=historical_data,\n",
    "        DB_fullpath=DB_path,\n",
    "        errorfile_fullpath=cpu_error\n",
    "    )\n",
    "\n",
    "    # Í≤∞Í≥º ÎπÑÍµê\n",
    "    Paper_Contamination_Compare(gpu_result, cpu_result)\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "run_contamination_dual()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc0c81-264a-4967-af32-909cf6d198cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import torch\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    # PDF -> Ïù¥ÎØ∏ÏßÄ\n",
    "    pages = convert_from_path(pdf_path, dpi=400, poppler_path=poppler_path)\n",
    "\n",
    "    # Î¶¨Îçî ÏÉùÏÑ±\n",
    "    reader_cpu = easyocr.Reader(['en', 'ko'], gpu=False, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    reader_gpu = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        image_array = np.array(page)\n",
    "\n",
    "        # CPU OCR\n",
    "        try:\n",
    "            text_cpu = reader_cpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"cpu\"].append(\"\\n\".join(text_cpu))\n",
    "        except Exception as e:\n",
    "            results[\"cpu\"].append(f\"[CPU OCR Ïã§Ìå®: {str(e)}]\")\n",
    "\n",
    "        # GPU OCR\n",
    "        try:\n",
    "            text_gpu = reader_gpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"gpu\"].append(\"\\n\".join(text_gpu))\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                results[\"gpu\"].append(\"[GPU OCR Ïã§Ìå® - OOM]\")\n",
    "            else:\n",
    "                results[\"gpu\"].append(f\"[GPU OCR Ïã§Ìå®: {str(e)}]\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()  # GPU Ï∫êÏãú ÎπÑÏö∞Í∏∞\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_result, gpu_result):\n",
    "    for i, (cpu, gpu) in enumerate(zip(cpu_result, gpu_result)):\n",
    "        print(f\"\\nüìù Page {i+1}\")\n",
    "        print(\"üîµ CPU:\")\n",
    "        print(cpu)\n",
    "        print(\"üü¢ GPU:\")\n",
    "        print(gpu)\n",
    "\n",
    "# üîΩ Ïó¨Í∏∞Ïóê PDF Í≤ΩÎ°úÎßå ÎÑ£ÏúºÎ©¥ Ïã§ÌñâÎê®\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[Í≥†Î¶¨4Ìò∏Í∏∞]Ï¶ùÍ∏∞Î∞úÏÉùÍ∏∞ 'A' ÏàòÏã§ Î∞∞ÏàòÍ¥ÄÏùò Î∞∞ÏàòÎ∞∏Î∏å Ïö©Ï†ëÎ∂Ä ÎàÑÏÑ§Î∂ÄÏúÑ Ï†ïÎπÑÎ•º ÏúÑÌïú ÏõêÏûêÎ°ú ÏàòÎèôÏ†ïÏßÄ.pdf\"\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03603d2c-61ed-418b-835c-623dfd166614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "import easyocr\n",
    "\n",
    "POPPLER_PATH = \"/usr/bin\"\n",
    "\n",
    "# Í∏∞Ï°¥ OCR Ìï®Ïàò Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ\n",
    "def paper_ocr(paper_name):\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    \n",
    "    if paper_name.endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            return [[f.read()]]  # Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Î∞òÌôò\n",
    "\n",
    "    elif paper_name.endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, dpi=800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=False)  # Í∏∞Ï°¥ ÏΩîÎìúÏôÄ ÏùºÏπò\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "# Í∏∞Ï°¥ OCR Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ TXT ÌååÏùºÎ°ú Î≥ÄÌôò\n",
    "def convert_pdf_to_txt(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_names = [f for f in os.listdir(input_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in file_names:\n",
    "        pdf_path = os.path.join(input_folder, pdf_file)\n",
    "        txt_filename = pdf_file.replace('.pdf', '.txt')\n",
    "        txt_path = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "        # OCR Ïã§Ìñâ (Í∏∞Ï°¥ paper_ocrÍ≥º ÎèôÏùºÌïú Î∞©Ïãù)\n",
    "        try:\n",
    "            ocr_result = paper_ocr(pdf_path)\n",
    "            extracted_text = [\"\\n\".join(page) for page in ocr_result]\n",
    "\n",
    "            # Í∏∞Ï°¥ Î¨∏Ïû• Íµ¨Ï°∞ÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ï†ÄÏû•\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                for page_num, text in enumerate(extracted_text, start=1):\n",
    "                    f.write(f\"[PAGE {page_num}]\\n{text}\\n\\n\")  # Í∏∞Ï°¥ Î∞©Ïãù Ïú†ÏßÄ\n",
    "\n",
    "            print(f\"‚úÖ {pdf_file} ‚Üí {txt_filename} Î≥ÄÌôò ÏôÑÎ£å\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {pdf_file} Î≥ÄÌôò Ïã§Ìå®: {e}\")\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "input_folder = \"paper_folder\"\n",
    "output_folder = os.path.join(input_folder, \"txt_output\")\n",
    "convert_pdf_to_txt(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b70540-0663-4e3f-9456-3e02698bd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "try:\n",
    "    llm = Llama(\n",
    "        model_path=GGUF_MODEL_PATH, \n",
    "        n_ctx=2048, \n",
    "        n_gpu_layers=0,  # GPU ÎØ∏ÏÇ¨Ïö©\n",
    "        n_batch=128,  # Î∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏù¥Í∏∞\n",
    "        n_threads=4   # CPU Ïä§Î†àÎìú Ïàò Ï†úÌïú\n",
    "    )\n",
    "    print(\"‚úÖ GGUF Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GGUF Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}\")\n",
    "import os\n",
    "print(os.path.exists(GGUF_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a223fb4-3260-4805-a3dd-3d024882cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌòïÌÉúÏÜå\tNNG,*,F,ÌòïÌÉúÏÜå,Compound,*,*,ÌòïÌÉú/NNG/*+ÏÜå/NNG/*\n",
      "Î∂ÑÏÑù\tNNG,ÌñâÏúÑ,T,Î∂ÑÏÑù,*,*,*,*\n",
      "Ïù¥\tJKS,*,F,Ïù¥,*,*,*,*\n",
      "Ïûò\tMAG,*,T,Ïûò,*,*,*,*\n",
      "Îêò\tVV,*,F,Îêò,*,*,*,*\n",
      "ÎÇòÏöî\tEF,*,F,ÎÇòÏöî,*,*,*,*\n",
      "?\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "print(tagger.parse(\"ÌòïÌÉúÏÜå Î∂ÑÏÑùÏù¥ Ïûò ÎêòÎÇòÏöî?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f03578-2a7a-47ca-9eeb-e6b451893aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from llama_cpp import Llama  # GGUF Î™®Îç∏ Ïã§ÌñâÏùÑ ÏúÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "import difflib\n",
    "\n",
    "# GGUF Î™®Îç∏ ÏÑ§Ï†ï (Mistral-7B GGUF ÏÇ¨Ïö©)\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q2_K_M.gguf\"\n",
    "\n",
    "# GGUF Î™®Îç∏ ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏\n",
    "if not os.path.exists(GGUF_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"GGUF Î™®Îç∏Ïù¥ {GGUF_MODEL_PATH}Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. Îã§Ïö¥Î°úÎìú ÌõÑ Îã§Ïãú Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
    "\n",
    "# GGUF Î™®Îç∏ Î°úÎìú (CPU Ïã§Ìñâ)\n",
    "llm = Llama(model_path=GGUF_MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "save_fullpath = \"keyword_llm_gguf.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList_llm_gguf.txt\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    \"\"\"\n",
    "    PDF ÎòêÎäî TXT ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌïòÎäî OCR Ìï®Ïàò.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=False)\n",
    "\n",
    "    # .txt ÌååÏùº Ï≤òÎ¶¨\n",
    "    if paper_name.lower().endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            paper_temp = [[f.read()]]\n",
    "        return paper_temp\n",
    "\n",
    "    # .pdf ÌååÏùº Ï≤òÎ¶¨ (OCR)\n",
    "    elif paper_name.lower().endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, 800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "    return []\n",
    "\n",
    "def extract_contamination_with_llm(data, DB_room_df, DB_mc_df):\n",
    "    \"\"\"\n",
    "    GGUF Í∏∞Î∞ò LLMÏùÑ Ïù¥Ïö©ÌïòÏó¨ Í≤©Ïã§ Î∞è Í∏∞Í∏∞Î™Ö Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌïòÍ≥† DBÏôÄ Îß§ÌïëÌïòÎäî Ìï®Ïàò.\n",
    "    \n",
    "    :param data: OCRÏóêÏÑú Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ (ÌéòÏù¥ÏßÄÎ≥Ñ Î¶¨Ïä§Ìä∏)\n",
    "    :param DB_room_df: Í≤©Ïã§ DB\n",
    "    :param DB_mc_df: Í∏∞Í∏∞ DB\n",
    "    :return: [{ÌååÏùºÏù¥Î¶Ñ, Î¨∏Ïû•, ÌéòÏù¥ÏßÄ, Î¨∏Ïû•ÏúÑÏπò, Í≤©Ïã§, Í∏∞Í∏∞Î™Ö, Ïû•ÏÜåÌëúÌòÑ, Í∏∞Í∏∞ÌëúÌòÑ}...]\n",
    "    \"\"\"\n",
    "    extracted_results = []\n",
    "\n",
    "    # DB Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    room_num = DB_room_df['Í≤©Ïã§Î≤àÌò∏'].astype(str).tolist()\n",
    "    room_en = DB_room_df['ÏòÅÎ¨∏Î™Ö'].astype(str).tolist()\n",
    "    room_ko = DB_room_df['ÌïúÍ∏ÄÎ™Ö'].astype(str).tolist()\n",
    "    mc_num = DB_mc_df['Í∏∞Îä•ÏúÑÏπò'].astype(str).tolist()\n",
    "    mc_name = DB_mc_df['Í∏∞Îä•ÏúÑÏπòÎ™Ö'].astype(str).tolist()\n",
    "    mc_room = DB_mc_df['ÏÑ§ÏπòÎ£∏'].astype(str).tolist()\n",
    "\n",
    "    for page_idx, page_content in enumerate(data):\n",
    "        text_block = \" \".join(page_content)\n",
    "\n",
    "        # GGUF Î™®Îç∏ÏùÑ Ïù¥Ïö©Ìïú LLM Ïã§Ìñâ\n",
    "        try:\n",
    "            response = llm(text_block, max_tokens=256)\n",
    "            if \"choices\" in response and isinstance(response[\"choices\"], list) and len(response[\"choices\"]) > 0:\n",
    "                extracted_text = response[\"choices\"][0][\"text\"]\n",
    "            else:\n",
    "                extracted_text = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Ï≤òÎ¶¨ Ïò§Î•ò: {e}\")\n",
    "            continue\n",
    "\n",
    "        # DBÏôÄ Îß§Ïπ≠ ÏàòÌñâ\n",
    "        matched_sentences = []\n",
    "        for sentence in extracted_text.split(\"\\n\"):\n",
    "            matched_row = {\"Î¨∏Ïû•\": sentence.strip(), \"ÌéòÏù¥ÏßÄ\": page_idx + 1, \"Î¨∏Ïû•ÏúÑÏπò\": len(matched_sentences) + 1, \"Ïû•ÏÜåÎ™Ö\": \"\", \"Í∏∞Í∏∞Î™Ö\": \"\", \"Ïû•ÏÜåÌëúÌòÑ\": \"\", \"Í∏∞Í∏∞ÌëúÌòÑ\": \"\"}\n",
    "\n",
    "            # Í≤©Ïã§Î™Ö Îß§Ïπ≠\n",
    "            for word in room_ko + room_en + room_num:\n",
    "                if word in sentence:\n",
    "                    matched_row[\"Ïû•ÏÜåÎ™Ö\"] = word\n",
    "                    matched_row[\"Ïû•ÏÜåÌëúÌòÑ\"] = word\n",
    "\n",
    "            # Í∏∞Í∏∞Î™Ö Îß§Ïπ≠\n",
    "            for word in mc_name + mc_num:\n",
    "                if word in sentence:\n",
    "                    matched_row[\"Í∏∞Í∏∞Î™Ö\"] = word\n",
    "                    matched_row[\"Í∏∞Í∏∞ÌëúÌòÑ\"] = word\n",
    "\n",
    "            matched_sentences.append(matched_row)\n",
    "\n",
    "        extracted_results.extend(matched_sentences)\n",
    "\n",
    "    return extracted_results\n",
    "\n",
    "# Ïã§Ìñâ Î∂ÄÎ∂Ñ\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# DB Î°úÎìú\n",
    "DB_room_df = pd.read_excel(DB_fullpath, sheet_name='Í≤©Ïã§ DB')\n",
    "DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='Í∏∞Í∏∞ DB')\n",
    "\n",
    "result_line_list = []\n",
    "errorfile = []\n",
    "\n",
    "for paper in file_names:\n",
    "    paper_name = os.path.join(folder_path, paper)\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "    except Exception as e:\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n",
    "\n",
    "    extracted_results = extract_contamination_with_llm(data, DB_room_df, DB_mc_df)\n",
    "\n",
    "    for row in extracted_results:\n",
    "        row[\"ÌååÏùºÏù¥Î¶Ñ\"] = paper_name\n",
    "        result_line_list.append(row)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "result_df = pd.DataFrame(result_line_list)\n",
    "result_df.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Ïò§Î•ò ÌååÏùº Î™©Î°ù Ï†ÄÏû•\n",
    "with open(errorfile_fullpath, \"w\") as file:\n",
    "    for item in errorfile:\n",
    "        file.write(str(item) + \"\\n\")\n",
    "\n",
    "print(\"GGUF Í∏∞Î∞ò LLM Í≤©Ïã§ Î∞è Í∏∞Í∏∞Î™Ö Ï∂îÏ∂ú ÏôÑÎ£å.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d96c38-f4a2-4165-969f-5317a1acd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# GGUF Î™®Îç∏ ÏÑ§Ï†ï\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# GGUF Î™®Îç∏ ÌååÏùº ÌôïÏù∏\n",
    "if not os.path.exists(GGUF_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"GGUF Î™®Îç∏Ïù¥ {GGUF_MODEL_PATH}Ïóê ÏóÜÏäµÎãàÎã§. Îã§Ïö¥Î°úÎìú ÌõÑ Îã§Ïãú Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
    "\n",
    "# GGUF Î™®Îç∏ Î°úÎìú (CPU Ïã§Ìñâ)\n",
    "llm = Llama(model_path=GGUF_MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "# PDF OCR Í¥ÄÎ†® ÏÑ§Ï†ï\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "save_fullpath = \"keyword_llm_gguf.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList_llm_gguf.txt\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    \"\"\"PDF ÎòêÎäî TXT ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌïòÎäî OCR Ìï®Ïàò\"\"\"\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=False)\n",
    "\n",
    "    if paper_name.lower().endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            return [[f.read()]]\n",
    "\n",
    "    elif paper_name.lower().endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, 800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "    return []\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"LLMÏù¥ Î∞òÌôòÌïú ÌÖçÏä§Ìä∏ÏóêÏÑú JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂úÌïòÎäî Ìï®Ïàò\"\"\"\n",
    "    match = re.search(r'\\{[\\s\\S]*?\\}', text)  # JSON Ìå®ÌÑ¥ Ï∞æÍ∏∞\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None  # JSONÏù¥ ÏóÜÏúºÎ©¥ None Î∞òÌôò\n",
    "\n",
    "def extract_contamination_with_llm(data, DB_room_df, DB_mc_df):\n",
    "    \"\"\"\n",
    "    GGUF Í∏∞Î∞ò LLMÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÏõêÏûêÎ†• Í¥ÄÎ†® Í≤©Ïã§ Î∞è Í∏∞Í∏∞Î™Ö Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌïòÍ≥† DBÏôÄ Îß§Ïπ≠ÌïòÎäî Ìï®Ïàò.\n",
    "    \"\"\"\n",
    "    extracted_results = []\n",
    "\n",
    "    # DBÏóêÏÑú Ïû•ÏÜåÎ™ÖÍ≥º Í∏∞Í∏∞Î™Ö Î¶¨Ïä§Ìä∏ ÏÉùÏÑ± (Í≤∞Ï∏°Ïπò Ï†úÍ±∞ ÌõÑ Î≥ÄÌôò)\n",
    "    room_list = DB_room_df[\"Ïû•ÏÜåÎ™Ö\"].dropna().astype(str).tolist()\n",
    "    mc_list = DB_mc_df[\"Í∏∞Í∏∞Î™Ö\"].dropna().astype(str).tolist()\n",
    "\n",
    "    for page_idx, page_content in enumerate(data):\n",
    "        for sentence in page_content:  # Î¨∏Ïû• Îã®ÏúÑÎ°ú LLM Ïã§Ìñâ\n",
    "            sentence = str(sentence).strip()  # Î¨∏Ïû•Ïù¥ NaNÏù¥Í±∞ÎÇò floatÏù∏ Í≤ΩÏö∞ ÎåÄÎπÑ\n",
    "\n",
    "            # 1. Î¨∏Ïû•ÏóêÏÑú DBÏùò Ïû•ÏÜåÎ™Ö/Í∏∞Í∏∞Î™ÖÏùÑ Ï∞æÍ∏∞\n",
    "            matched_rooms = [room for room in room_list if room in sentence]\n",
    "            matched_mcs = [mc for mc in mc_list if mc in sentence]\n",
    "\n",
    "            # 2. Ïû•ÏÜåÎ™Ö/Í∏∞Í∏∞Î™ÖÏù¥ ÏóÜÏúºÎ©¥ LLM Ìò∏Ï∂úÌïòÏßÄ ÏïäÏùå\n",
    "            if not matched_rooms and not matched_mcs:\n",
    "                continue\n",
    "\n",
    "            # 3. LLM ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑú JSON Ï∂úÎ†• Í∞ïÏ†ú\n",
    "            prompt = f\"\"\"\n",
    "            üîπ Î¨∏Ïû•ÏóêÏÑú Ïû•ÏÜåÎ™ÖÍ≥º Í∏∞Í∏∞Î™ÖÏùÑ JSON ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•ÌïòÏÑ∏Ïöî.\n",
    "\n",
    "            **üìå Ï£ºÏùòÏÇ¨Ìï≠**\n",
    "            - **JSON Ïô∏Ïùò ÏÑ§Î™ÖÏùÑ Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.**\n",
    "            - Ïû•ÏÜåÎ™ÖÏùÄ 'Í≤©Ïã§ DB'Ïùò 'ÌïúÍ∏ÄÎ™Ö' Ïª¨ÎüºÏóêÏÑú Í∞ÄÏ†∏Ïò® Í∞íÍ≥º ÏùºÏπòÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "            - Í∏∞Í∏∞Î™ÖÏùÄ 'Í∏∞Í∏∞ DB'Ïùò 'Í∏∞Îä•ÏúÑÏπòÎ™Ö' Ïª¨ÎüºÏóêÏÑú Í∞ÄÏ†∏Ïò® Í∞íÍ≥º ÏùºÏπòÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "            **üìú Î¨∏Ïû• ÏòàÏãú**\n",
    "            \"{sentence}\"\n",
    "\n",
    "            **‚úÖ JSON ÌòïÏãù ÏòàÏ†ú**\n",
    "            ```json\n",
    "            {{\n",
    "                \"Ïû•ÏÜåÎ™Ö\": \"{matched_rooms[0] if matched_rooms else 'ÎØ∏ÌôïÏù∏'}\",\n",
    "                \"Í∏∞Í∏∞Î™Ö\": \"{matched_mcs[0] if matched_mcs else 'ÎØ∏ÌôïÏù∏'}\",\n",
    "                \"Ïû•ÏÜåÌëúÌòÑ\": \"ÏõêÎ¨∏ÏóêÏÑú Ï∂îÏ∂úÎêú ÌëúÌòÑ\",\n",
    "                \"Í∏∞Í∏∞ÌëúÌòÑ\": \"ÏõêÎ¨∏ÏóêÏÑú Ï∂îÏ∂úÎêú ÌëúÌòÑ\"\n",
    "            }}\n",
    "            ```\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = llm.create_completion(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=500,  # Ï∂©Î∂ÑÌïú ÏùëÎãµ Í≥µÍ∞Ñ ÌôïÎ≥¥\n",
    "                    stop=[\"\\n\\n\", \"```\"]  # ÏΩîÎìú Î∏îÎ°ù Ï¢ÖÎ£å Í∞ïÏ†ú\n",
    "                )\n",
    "                extracted_text = response[\"choices\"][0][\"text\"] if \"choices\" in response else \"\"\n",
    "\n",
    "                # JSON Ï∂îÏ∂ú Î∞è Í≤ÄÏ¶ù\n",
    "                json_text = extract_json_from_text(extracted_text)\n",
    "\n",
    "                if json_text:\n",
    "                    try:\n",
    "                        extracted_info = json.loads(json_text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"‚ö†Ô∏è JSON Î≥ÄÌôò Ïò§Î•ò Î∞úÏÉù. ÏõêÎ≥∏ Ï∂úÎ†•: {json_text}\")\n",
    "                        extracted_info = {}\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è JSON ÌòïÏãù Í∞êÏßÄ Ïã§Ìå®. LLM ÏùëÎãµ Î¨∏Ï†ú Í∞ÄÎä•ÏÑ± ÏûàÏùå.\")\n",
    "                    extracted_info = {}\n",
    "\n",
    "                # Í≤∞Í≥º Ï†ïÎ¶¨\n",
    "                matched_row = {\n",
    "                    \"Î¨∏Ïû•\": sentence.strip(),\n",
    "                    \"ÌéòÏù¥ÏßÄ\": page_idx + 1,\n",
    "                    \"Î¨∏Ïû•ÏúÑÏπò\": len(extracted_results) + 1,\n",
    "                    \"Ïû•ÏÜåÎ™Ö\": extracted_info.get(\"Ïû•ÏÜåÎ™Ö\", \"\"),\n",
    "                    \"Í∏∞Í∏∞Î™Ö\": extracted_info.get(\"Í∏∞Í∏∞Î™Ö\", \"\"),\n",
    "                    \"Ïû•ÏÜåÌëúÌòÑ\": extracted_info.get(\"Ïû•ÏÜåÌëúÌòÑ\", \"\"),\n",
    "                    \"Í∏∞Í∏∞ÌëúÌòÑ\": extracted_info.get(\"Í∏∞Í∏∞ÌëúÌòÑ\", \"\")\n",
    "                }\n",
    "                extracted_results.append(matched_row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LLM Ï≤òÎ¶¨ Ïò§Î•ò: {e}\")\n",
    "                continue\n",
    "\n",
    "    return extracted_results\n",
    "\n",
    "# Ïã§Ìñâ Î∂ÄÎ∂Ñ\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# DB Î∂àÎü¨Ïò§Í∏∞\n",
    "DB_room_df = pd.read_excel(DB_fullpath, sheet_name='Í≤©Ïã§ DB')\n",
    "DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='Í∏∞Í∏∞ DB')\n",
    "\n",
    "# Ïª¨ÎüºÎ™Ö ÏûêÎèô ÌôïÏù∏ ÌõÑ Îß§Ìïë\n",
    "DB_room_df.rename(columns={\"ÌïúÍ∏ÄÎ™Ö\": \"Ïû•ÏÜåÎ™Ö\"}, inplace=True)\n",
    "DB_mc_df.rename(columns={\"Í∏∞Îä•ÏúÑÏπòÎ™Ö\": \"Í∏∞Í∏∞Î™Ö\"}, inplace=True)\n",
    "\n",
    "result_line_list = []\n",
    "errorfile = []\n",
    "\n",
    "for paper in file_names:\n",
    "    paper_name = os.path.join(folder_path, paper)\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "    except Exception as e:\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n",
    "\n",
    "    extracted_results = extract_contamination_with_llm(data, DB_room_df, DB_mc_df)\n",
    "\n",
    "    for row in extracted_results:\n",
    "        row[\"ÌååÏùºÏù¥Î¶Ñ\"] = paper_name\n",
    "        result_line_list.append(row)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "result_df = pd.DataFrame(result_line_list)\n",
    "result_df.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"‚úÖ GGUF Í∏∞Î∞ò LLM Í≤©Ïã§ Î∞è Í∏∞Í∏∞Î™Ö Ï∂îÏ∂ú ÏôÑÎ£å.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb12df-7bfc-4e4e-92cd-0762cee5991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)  # torch Î≤ÑÏ†Ñ Ï∂úÎ†•\n",
    "print(torchvision.__version__)  # torchvision Î≤ÑÏ†Ñ Ï∂úÎ†•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ba815-68b0-47bd-92df-aa8d98fc12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "sentences = [\"ÏïàÎÖïÌïòÏÑ∏Ïöî\", \"Ïù¥ Î¨∏Ïû•ÏùÄ ÌÖåÏä§Ìä∏Î•º ÏúÑÌïú Í≤ÉÏûÖÎãàÎã§.\"]\n",
    "embeddings = embedder.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Îëê Î¨∏Ïû•Ïùò Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5bde2-a45c-4ae1-87e0-5921cb2eb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "model = AutoModel.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Î¨∏Ïû•\n",
    "text = \"ÌïúÍµ≠Ïñ¥ Î¨∏Ïû•ÏùÑ ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Ï∂úÎ†• ÌÖêÏÑú ÌôïÏù∏\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2474c-b4c0-4e99-827f-1fca25598603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jung_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
