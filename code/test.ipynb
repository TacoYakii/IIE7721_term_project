{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af263168-66d3-4388-9492-2bfd6aec9d6e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# DPI 1000\n",
    "from ContaminationExtractor import Paper_Contamination \n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "save_fullpath = \"keyword_100000.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList.txt\"\n",
    "\n",
    "\n",
    "\n",
    "Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18c505c-3b70-4a50-bb15-e6879e3c4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TotalFiles\": \"100\", \"ExecutionTimeInSeconds\": \"4,364\"}\n"
     ]
    }
   ],
   "source": [
    "# DPI 1000\n",
    "from ContaminationExtractor import Paper_Contamination \n",
    "\n",
    "\n",
    "save_fullpath = \"keyword_500000.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList.txt\"\n",
    "\n",
    "\n",
    "\n",
    "Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a8a1fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfile_names\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.DS_Store\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"
     ]
    }
   ],
   "source": [
    "for paper in file_names:\n",
    "    if paper == '.DS_Store':\n",
    "        continue\n",
    "    print(f\"🧾 OCR 시작: {paper}\")\n",
    "    paper_name = folder_path + '/' + paper\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "        print(f\"✅ OCR 성공: {paper} → 총 페이지 수 {len(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ OCR 실패: {paper} → {e}\")\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cf31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Paper_Contamination(save_fullpath, folder_path, historical_data_fullpath, DB_fullpath, errorfile_fullpath):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from paper_ocr import paper_ocr\n",
    "    import kss\n",
    "    import re\n",
    "    import os\n",
    "    import warnings\n",
    "    import time\n",
    "    import json\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import difflib\n",
    "    from tqdm import tqdm\n",
    "    from difflib import SequenceMatcher\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    # 경고 무시\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Sentence DB\n",
    "    historical_data = pd.read_excel(\n",
    "        historical_data_fullpath, engine=\"openpyxl\")\n",
    "    queries = historical_data.values.tolist()\n",
    "\n",
    "    # 격실 및 기기 DB\n",
    "    DB_room_df = pd.read_excel(DB_fullpath, sheet_name='격실 DB')\n",
    "    DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='기기 DB')\n",
    "\n",
    "    # 폴더 내의 파일 이름 가져오기 (첫 두 개의 파일만 처리)\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "            file_names.append(file_name)\n",
    "    file_names = file_names[:2]  # 첫 두 개 파일만 처리\n",
    "\n",
    "    # Mistral 모델 로드\n",
    "    mistral_model_path = \"/home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "    llm = Llama(model_path=mistral_model_path, n_ctx=2048,\n",
    "                n_threads=8, n_gpu_layers=30, verbose=False)\n",
    "\n",
    "    result_line_list = []\n",
    "    errorfile = []\n",
    "\n",
    "    for paper in file_names:\n",
    "        if paper == '.DS_Store':\n",
    "            continue\n",
    "        paper_name = folder_path + '/' + paper\n",
    "\n",
    "        # OCR\n",
    "        try:\n",
    "            data = paper_ocr(paper_name)\n",
    "        except:\n",
    "            errorfile.append(paper_name)\n",
    "            continue\n",
    "\n",
    "        # 공백 제거\n",
    "        newdata = []\n",
    "        for page in data:\n",
    "            preprocessed_page = [\n",
    "                re.sub(' +', ' ', sentence).strip() for sentence in page]\n",
    "            newdata.append(preprocessed_page)\n",
    "\n",
    "        # 문장 분리\n",
    "        sentence_temp = []\n",
    "        if paper_name[-3:].lower() == 'txt' or paper_name[-3:].lower() == 'pdf':\n",
    "            for j_idx, j in enumerate(newdata):\n",
    "                sentences = []\n",
    "                for item in j:\n",
    "                    sentences.extend(kss.split_sentences(item))\n",
    "                for sentence_idx, sentence in enumerate(sentences):\n",
    "                    sentence_temp.append(\n",
    "                        {'문장': sentence, '페이지': j_idx + 1, '문장위치': sentence_idx + 1})\n",
    "\n",
    "        # 10자 이하 문장 제거\n",
    "        min_length = 10\n",
    "        sentence_temp = [s for s in sentence_temp if len(s['문장']) > min_length]\n",
    "        sentence = pd.DataFrame(sentence_temp)\n",
    "        sentence_list = [item['문장'] for item in sentence_temp]\n",
    "\n",
    "        # 결과 비교\n",
    "        result_df_list = []\n",
    "        similarity_cache = {}\n",
    "\n",
    "        for query_row in queries:\n",
    "            query = query_row[0]\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "                futures = []\n",
    "\n",
    "                for target in sentence_list:\n",
    "                    key = (query, target)\n",
    "                    if key in similarity_cache:\n",
    "                        futures.append(executor.submit(\n",
    "                            lambda q=query, t=target: (q, t, similarity_cache[key])))\n",
    "                    else:\n",
    "                        simple_similarity = SequenceMatcher(\n",
    "                            None, query, target).ratio()\n",
    "                        if simple_similarity < 0.2:\n",
    "                            continue\n",
    "                        futures.append(executor.submit(\n",
    "                            lambda q=query, t=target: (\n",
    "                                q, t,\n",
    "                                float(llm(f\"\"\"다음 두 문장은 얼마나 유사합니까? 0부터 1 사이의 점수로 정수나 소수로만 답해주세요.\n",
    "문장1: {q}\n",
    "문장2: {t}\n",
    "점수:\"\"\", max_tokens=10, stop=[\"\\n\"])['choices'][0]['text'].strip())\n",
    "                            )\n",
    "                        ))\n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            q, t, score = future.result()\n",
    "                            similarity_cache[(q, t)] = score\n",
    "                            if score > 0.3:\n",
    "                                result_df_list.append(pd.DataFrame({\n",
    "                                    'Query': [q],\n",
    "                                    '오염문장': [t.strip()],\n",
    "                                    'score': [score]\n",
    "                                }))\n",
    "                        except:\n",
    "                            continue\n",
    "                        if len(result_df_list) == 0:\n",
    "                            continue\n",
    "\n",
    "        # 결과 DataFrame 생성\n",
    "        if result_df_list:\n",
    "            result = pd.concat(result_df_list, ignore_index=True)\n",
    "            result = result.sort_values(by='score', ascending=False)\n",
    "            result_fin = result.drop_duplicates(['오염문장'], keep='first')\n",
    "            result_similarity = result_fin.reset_index(drop=True)\n",
    "\n",
    "            # 페이지, 문장위치 추가\n",
    "            result_similarity['페이지'] = None\n",
    "            result_similarity['문장위치'] = None\n",
    "\n",
    "            for index, row in result_similarity.iterrows():\n",
    "                query = row['오염문장']\n",
    "                match_row = sentence[sentence['문장'] == query]\n",
    "                if not match_row.empty:\n",
    "                    result_similarity.at[index, '페이지'] = match_row['페이지'].values[0]\n",
    "                    result_similarity.at[index,\n",
    "                                         '문장위치'] = match_row['문장위치'].values[0]\n",
    "\n",
    "            # 결과 출력\n",
    "            print(f\"Paper: {paper}\")\n",
    "            print(result_similarity[['Query', '오염문장', 'score']].head())  # 첫 5개 결과 출력\n",
    "\n",
    "    # 결과를 CSV로 저장\n",
    "    result_line = pd.DataFrame(result_line_list)\n",
    "    try:\n",
    "        result_line = result_line.sort_values(by='score', ascending=False)\n",
    "    except:\n",
    "        pass\n",
    "    result_line = result_line.drop_duplicates(['오염문장', '장소표현'], keep='first')\n",
    "    try:\n",
    "        result_line = result_line.drop(columns=['score'])\n",
    "    except:\n",
    "        pass\n",
    "    result_line = result_line.reset_index(drop=True)\n",
    "\n",
    "    if result_line.empty:\n",
    "        result_line['파일이름'] = None\n",
    "        result_line['오염문장'] = None\n",
    "        result_line['페이지'] = None\n",
    "        result_line['문장위치'] = None\n",
    "        result_line['장소명'] = None\n",
    "        result_line['기기명'] = None\n",
    "        result_line['장소표현'] = None\n",
    "        result_line['기기표현'] = None\n",
    "\n",
    "    result_line.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # 오류 파일 기록\n",
    "    with open(errorfile_fullpath, 'w') as file:\n",
    "        for item in errorfile:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "    # 실행 종료 시간 기록\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 실행 시간 계산\n",
    "    execution_time = end_time - start_time\n",
    "    total_files = len(file_names)\n",
    "\n",
    "    # JSON 형식으로 출력\n",
    "    JSON_text = {\n",
    "        \"TotalFiles\": f\"{total_files:,}\",\n",
    "        \"ExecutionTimeInSeconds\": f\"{execution_time:,.0f}\"\n",
    "    }\n",
    "\n",
    "    json_data = json.dumps(JSON_text)\n",
    "    print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a544dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 테스트 예시 (경로 수정해서 사용하세요)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpaper_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/taco/Documents/projects/jung/model/paper_folder/[고리4호기]증기발생기 \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m 수실 배수관의 배수밸브 용접부 누설부위 정비를 위한 원자로 수동정지.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 결과 확인\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mpaper_ocr\u001b[0;34m(paper_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[1;32m     35\u001b[0m         image_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(page)\n\u001b[0;32m---> 36\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         paper_temp\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paper_temp\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[0;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[1;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_cv_grey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizontal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocklist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[0;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[1;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[0;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mignore_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/recognition.py:206\u001b[0m, in \u001b[0;36mget_text\u001b[0;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    202\u001b[0m     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_normal, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# predict first round\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mignore_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_group_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# predict second round\u001b[39;00m\n\u001b[1;32m    210\u001b[0m low_confident_idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result1) \u001b[38;5;28;01mif\u001b[39;00m (item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m contrast_ths)]\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/recognition.py:111\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[0;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m length_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([batch_max_length] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    109\u001b[0m text_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size, batch_max_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 111\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_for_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Select max probabilty (greedy decoding) then decode index to character\u001b[39;00m\n\u001b[1;32m    114\u001b[0m preds_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m batch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/model/vgg_model.py:25\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input, text)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, text):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Feature extraction stage \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatureExtraction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool(visual_feature\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     27\u001b[0m     visual_feature \u001b[38;5;241m=\u001b[39m visual_feature\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/easyocr/model/modules.py:124\u001b[0m, in \u001b[0;36mVGG_FeatureExtractor.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jung_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import traceback\n",
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/taco/Documents/projects/jung/poppler_2301_build/lib\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    try:\n",
    "        # GPU 사용 설정, 모델 경로 명시\n",
    "        reader = easyocr.Reader(\n",
    "            ['en', 'ko'],\n",
    "            gpu=False,\n",
    "            model_storage_directory='model',\n",
    "            user_network_directory='model',\n",
    "            download_enabled=False\n",
    "        )\n",
    "\n",
    "        # .txt 파일 처리\n",
    "        if paper_name[-3:].lower() == 'txt':\n",
    "            with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "                paper_temp = f.read()\n",
    "                paper_temp = [[paper_temp]]\n",
    "\n",
    "        # .pdf 파일 처리\n",
    "        elif paper_name[-3:].lower() == 'pdf':\n",
    "            pages = convert_from_path(\n",
    "                paper_name,\n",
    "                dpi=800,\n",
    "                poppler_path=\"/home/taco/Documents/projects/jung/poppler_2301_build/bin\"  # 리눅스 poppler 경로 (환경에 따라 조정 가능)\n",
    "            )\n",
    "            paper_temp = []\n",
    "            for page in pages:\n",
    "                image_array = np.array(page)\n",
    "                result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "                paper_temp.append(result)\n",
    "\n",
    "        return paper_temp\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ 변환 실패!\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# 테스트 예시 (경로 수정해서 사용하세요)\n",
    "output = paper_ocr(\"/home/taco/Documents/projects/jung/model/paper_folder/[고리4호기]증기발생기 'A' 수실 배수관의 배수밸브 용접부 누설부위 정비를 위한 원자로 수동정지.pdf\")\n",
    "\n",
    "# 결과 확인\n",
    "if output is not None:\n",
    "    for i, page in enumerate(output):\n",
    "        print(f\"\\n--- Page {i+1} ---\")\n",
    "        print(\"\\n\".join(page))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77204ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3080 Laptop GPU) - 7619 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =   181.04 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 356 (with bs=512), 1 (with bs=1)\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "[Kss]: Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 선택된 파일: paper_folder/주증기모관 배기밸브 정비, 저압터빈 날개 점검, MSR Expansion Bellows Joint 점검 등을 위한 원자로 수....pdf\n",
      "🔄 LLM 모델 로드 중...\n",
      "✅ LLM 모델 로드 완료\n",
      "\n",
      "📥 Sentence DB 로딩 중...\n",
      "✅ 문장 쿼리 수: 136\n",
      "🔍 OCR 시작...\n",
      "✅ OCR 성공\n",
      "✅ OCR 후 페이지 수: 1\n",
      "✅ 분리된 문장 수: 19\n",
      "✅ 10자 이상 문장 수: 5\n",
      "\n",
      "🔬 LLM 유사도 테스트\n",
      "🔸 Query 문장:\n",
      "원자로건물 2층의 냉각수 저장탱크에서 누설이 발생하여 방사능 감지기가 작동하였다. 작업자 B가 탱크의 밸브를 잘못 조작한 것이 원인으로 파악되었다.\n",
      "🔹 Target 문장:\n",
      "사건발생일자 85/10/19 23:58\n",
      "🔁 SequenceMatcher 유사도: 0.058\n",
      "⛔ 유사도 낮아서 스킵\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import kss\n",
    "import traceback\n",
    "from llama_cpp import Llama\n",
    "from difflib import SequenceMatcher\n",
    "from paper_ocr import paper_ocr  # OCR 함수 사용 가능 상태여야 함\n",
    "\n",
    "# 🔧 설정\n",
    "folder_path = \"paper_folder\"\n",
    "historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "mistral_model_path = \"/home/taco/Documents/projects/jung/model/mis/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# 점수 추출 함수\n",
    "def extract_score(text):\n",
    "    try:\n",
    "        match = re.search(r\"([0-9]+\\.?[0-9]*)\", text)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Step 0. 첫 파일만 선택\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.pdf') and not f.startswith('.')]\n",
    "if not file_names:\n",
    "    raise FileNotFoundError(\"❌ PDF 파일이 없습니다.\")\n",
    "\n",
    "paper = file_names[0]\n",
    "paper_path = os.path.join(folder_path, paper)\n",
    "print(f\"\\n📄 선택된 파일: {paper_path}\")\n",
    "\n",
    "# Step 1. LLM 모델 로드\n",
    "print(\"🔄 LLM 모델 로드 중...\")\n",
    "llm = Llama(model_path=mistral_model_path, n_ctx=512, n_threads=4, n_gpu_layers=0, verbose=True)\n",
    "print(\"✅ LLM 모델 로드 완료\\n\")\n",
    "\n",
    "# Step 2. Sentence DB\n",
    "print(\"📥 Sentence DB 로딩 중...\")\n",
    "queries = pd.read_excel(historical_data_fullpath, engine=\"openpyxl\").values.tolist()\n",
    "print(f\"✅ 문장 쿼리 수: {len(queries)}\")\n",
    "\n",
    "# Step 3. OCR\n",
    "try:\n",
    "    print(\"🔍 OCR 시작...\")\n",
    "    data = paper_ocr(paper_path)\n",
    "    print(\"✅ OCR 성공\")\n",
    "except Exception as e:\n",
    "    print(\"❌ OCR 실패\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# Step 4. 공백 제거\n",
    "newdata = []\n",
    "for page in data:\n",
    "    preprocessed = [re.sub(' +', ' ', s).strip() for s in page]\n",
    "    newdata.append(preprocessed)\n",
    "print(f\"✅ OCR 후 페이지 수: {len(newdata)}\")\n",
    "\n",
    "# Step 5. 문장 분리\n",
    "sentence_temp = []\n",
    "for j_idx, page in enumerate(newdata):\n",
    "    sentences = []\n",
    "    for item in page:\n",
    "        sentences.extend(kss.split_sentences(item))\n",
    "    for sentence_idx, sentence in enumerate(sentences):\n",
    "        sentence_temp.append({'문장': sentence, '페이지': j_idx+1, '문장위치': sentence_idx+1})\n",
    "print(f\"✅ 분리된 문장 수: {len(sentence_temp)}\")\n",
    "\n",
    "# Step 6. 10자 이상 필터\n",
    "sentence_temp = [s for s in sentence_temp if len(s['문장']) > 10]\n",
    "sentence_list = [s['문장'] for s in sentence_temp]\n",
    "print(f\"✅ 10자 이상 문장 수: {len(sentence_list)}\")\n",
    "\n",
    "# Step 7. LLM 유사도 비교 (1개만)\n",
    "print(\"\\n🔬 LLM 유사도 테스트\")\n",
    "query = queries[0][0].strip()\n",
    "target = sentence_list[0].strip()\n",
    "\n",
    "print(f\"🔸 Query 문장:\\n{query}\")\n",
    "print(f\"🔹 Target 문장:\\n{target}\")\n",
    "\n",
    "sim_score = SequenceMatcher(None, query, target).ratio()\n",
    "print(f\"🔁 SequenceMatcher 유사도: {sim_score:.3f}\")\n",
    "if sim_score < 0.3:\n",
    "    print(\"⛔ 유사도 낮아서 스킵\")\n",
    "else:\n",
    "    prompt = f\"\"\"두 문장의 유사도를 0에서 1 사이 소수점 점수로만 숫자 하나만 출력하세요. 다른 말은 하지 마세요.\\n문장1: {query}\\n문장2: {target}\\n답:\"\"\"\n",
    "    try:\n",
    "        output = llm(prompt, max_tokens=10, stop=[\"\\n\"])\n",
    "        score = extract_score(output['choices'][0]['text'])\n",
    "        print(f\"✅ LLM 유사도 점수: {score}\")\n",
    "    except:\n",
    "        print(\"❌ LLM 호출 실패\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c98c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import difflib\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    # PDF → 이미지 변환\n",
    "    pages = convert_from_path(pdf_path, dpi=800, poppler_path=poppler_path)\n",
    "\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    for gpu_mode in [False, True]:\n",
    "        reader = easyocr.Reader(['en', 'ko'], gpu=gpu_mode,\n",
    "                                model_storage_directory='model',\n",
    "                                user_network_directory='model',\n",
    "                                download_enabled=False)\n",
    "        key = \"gpu\" if gpu_mode else \"cpu\"\n",
    "\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            text = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[key].append(\"\\n\".join(text))\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_output, gpu_output):\n",
    "    print(\"=== 🔍 Difference between CPU and GPU OCR ===\\n\")\n",
    "    for page_idx, (cpu_text, gpu_text) in enumerate(zip(cpu_output, gpu_output)):\n",
    "        print(f\"\\n--- Page {page_idx + 1} ---\")\n",
    "        cpu_lines = cpu_text.splitlines()\n",
    "        gpu_lines = gpu_text.splitlines()\n",
    "        diff = difflib.unified_diff(cpu_lines, gpu_lines, fromfile='CPU', tofile='GPU', lineterm='')\n",
    "        for line in diff:\n",
    "            print(line)\n",
    "\n",
    "# 사용 예시\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[고리4호기]증기발생기 'A' 수실 배수관의 배수밸브 용접부 누설부위 정비를 위한 원자로 수동정지.pdf\"\n",
    "\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from ContaminationExtractor import Paper_Contamination  # 외부 모듈 불러오기\n",
    "\n",
    "def Paper_Contamination_Compare(gpu_result_path, cpu_result_path):\n",
    "    print(\"\\n📊 [결과 비교 중]\")\n",
    "\n",
    "    gpu_df = pd.read_csv(gpu_result_path)\n",
    "    cpu_df = pd.read_csv(cpu_result_path)\n",
    "\n",
    "    gpu_sentences = set(gpu_df['오염문장'].dropna().str.strip().unique())\n",
    "    cpu_sentences = set(cpu_df['오염문장'].dropna().str.strip().unique())\n",
    "\n",
    "    print(f\"총 문장 수 (GPU): {len(gpu_sentences)}\")\n",
    "    print(f\"총 문장 수 (CPU): {len(cpu_sentences)}\")\n",
    "\n",
    "    gpu_only = gpu_sentences - cpu_sentences\n",
    "    cpu_only = cpu_sentences - gpu_sentences\n",
    "\n",
    "    print(f\"\\n🔍 GPU/CPU 결과에서 서로 다른 오염문장 수: {len(gpu_only) + len(cpu_only)}\")\n",
    "    if not gpu_only and not cpu_only:\n",
    "        print(\"✅ GPU와 CPU 결과는 **완전히 동일합니다.**\")\n",
    "    else:\n",
    "        print(\"❗ 차이 발생!\")\n",
    "        if gpu_only:\n",
    "            print(\"\\n🔺 GPU에만 있는 문장 (예시 최대 5개):\")\n",
    "            for sent in list(gpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "        if cpu_only:\n",
    "            print(\"\\n🔻 CPU에만 있는 문장 (예시 최대 5개):\")\n",
    "            for sent in list(cpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "\n",
    "\n",
    "def run_contamination_dual():\n",
    "    # 경로 설정 (사용자 입력 반영)\n",
    "    folder_path = \"paper_folder\"\n",
    "    historical_data_fullpath = \"Historical_data.xlsx\"\n",
    "    DB_fullpath = \"Model_DB.xlsx\"\n",
    "\n",
    "    # GPU 경로\n",
    "    gpu_result = \"keyword_100000_gpu.csv\"\n",
    "    gpu_error = \"ErrorFileList_gpu.txt\"\n",
    "\n",
    "    # CPU 경로\n",
    "    cpu_result = \"keyword_100000_cpu.csv\"\n",
    "    cpu_error = \"ErrorFileList_cpu.txt\"\n",
    "\n",
    "    # 1. GPU 실행\n",
    "    print(\"▶ GPU 실행 중...\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPU 지정\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=gpu_result,\n",
    "        folder_path=folder_path,\n",
    "        historical_data_fullpath=historical_data_fullpath,\n",
    "        DB_fullpath=DB_fullpath,\n",
    "        errorfile_fullpath=gpu_error\n",
    "    )\n",
    "\n",
    "    # 2. CPU 실행\n",
    "    print(\"\\n▶ CPU 실행 중...\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # CPU 강제 지정\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=cpu_result,\n",
    "        folder_path=folder_path,\n",
    "        historical_data_fullpath=historical_data_fullpath,\n",
    "        DB_fullpath=DB_fullpath,\n",
    "        errorfile_fullpath=cpu_error\n",
    "    )\n",
    "\n",
    "    # 3. 결과 비교\n",
    "    Paper_Contamination_Compare(gpu_result, cpu_result)\n",
    "\n",
    "# 실행\n",
    "run_contamination_dual()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551afd-b1af-4eb5-93b5-0b1e2fce4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import torch\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    # PDF -> 이미지\n",
    "    pages = convert_from_path(pdf_path, dpi=400, poppler_path=poppler_path)\n",
    "\n",
    "    # 리더 생성\n",
    "    reader_cpu = easyocr.Reader(['en', 'ko'], gpu=False, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    reader_gpu = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        image_array = np.array(page)\n",
    "\n",
    "        # CPU OCR\n",
    "        try:\n",
    "            text_cpu = reader_cpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"cpu\"].append(\"\\n\".join(text_cpu))\n",
    "        except Exception as e:\n",
    "            results[\"cpu\"].append(f\"[CPU OCR 실패: {str(e)}]\")\n",
    "\n",
    "        # GPU OCR\n",
    "        try:\n",
    "            text_gpu = reader_gpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"gpu\"].append(\"\\n\".join(text_gpu))\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                results[\"gpu\"].append(\"[GPU OCR 실패 - OOM]\")\n",
    "            else:\n",
    "                results[\"gpu\"].append(f\"[GPU OCR 실패: {str(e)}]\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()  # GPU 캐시 비우기\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_result, gpu_result):\n",
    "    for i, (cpu, gpu) in enumerate(zip(cpu_result, gpu_result)):\n",
    "        print(f\"\\n📝 Page {i+1}\")\n",
    "        print(\"🔵 CPU:\")\n",
    "        print(cpu)\n",
    "        print(\"🟢 GPU:\")\n",
    "        print(gpu)\n",
    "\n",
    "# 🔽 여기에 PDF 경로만 넣으면 실행됨\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[고리4호기]증기발생기 'A' 수실 배수관의 배수밸브 용접부 누설부위 정비를 위한 원자로 수동정지.pdf\"\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69070370-c0bc-4cb7-ae78-e19c8c2e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from contamination import Paper_Contamination  # 기존 함수 불러온다고 가정\n",
    "\n",
    "def Paper_Contamination_Compare(gpu_result_path, cpu_result_path):\n",
    "    print(\"\\n📊 [결과 비교 중]\")\n",
    "\n",
    "    gpu_df = pd.read_csv(gpu_result_path)\n",
    "    cpu_df = pd.read_csv(cpu_result_path)\n",
    "\n",
    "    gpu_sentences = set(gpu_df['오염문장'].dropna().str.strip().unique())\n",
    "    cpu_sentences = set(cpu_df['오염문장'].dropna().str.strip().unique())\n",
    "\n",
    "    print(f\"총 문장 수 (GPU): {len(gpu_sentences)}\")\n",
    "    print(f\"총 문장 수 (CPU): {len(cpu_sentences)}\")\n",
    "\n",
    "    gpu_only = gpu_sentences - cpu_sentences\n",
    "    cpu_only = cpu_sentences - gpu_sentences\n",
    "\n",
    "    print(f\"\\n🔍 GPU/CPU 결과에서 서로 다른 오염문장 수: {len(gpu_only) + len(cpu_only)}\")\n",
    "    if not gpu_only and not cpu_only:\n",
    "        print(\"✅ GPU와 CPU 결과는 **완전히 동일합니다.**\")\n",
    "    else:\n",
    "        print(\"❗ 차이 발생!\")\n",
    "        if gpu_only:\n",
    "            print(\"\\n🔺 GPU에만 있는 문장 (예시 최대 5개):\")\n",
    "            for sent in list(gpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "        if cpu_only:\n",
    "            print(\"\\n🔻 CPU에만 있는 문장 (예시 최대 5개):\")\n",
    "            for sent in list(cpu_only)[:5]:\n",
    "                print(\" -\", sent)\n",
    "\n",
    "\n",
    "def run_contamination_dual():\n",
    "    # 공통 설정\n",
    "    paper_folder = \"/home/taco/Documents/projects/jung/model/paper_folder\"\n",
    "    historical_data = \"Historical_data.xlsx\"\n",
    "    DB_path = \"Model_DB.xlsx\"\n",
    "    result_dir = \"/home/taco/Documents/projects/jung/model/results\"\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # GPU 실행\n",
    "    print(\"▶ GPU 실행 중...\")\n",
    "    gpu_result = os.path.join(result_dir, \"gpu_result.csv\")\n",
    "    gpu_error = os.path.join(result_dir, \"gpu_error.txt\")\n",
    "    torch.cuda.set_device(0) if torch.cuda.is_available() else None\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=gpu_result,\n",
    "        folder_path=paper_folder,\n",
    "        historical_data_fullpath=historical_data,\n",
    "        DB_fullpath=DB_path,\n",
    "        errorfile_fullpath=gpu_error\n",
    "    )\n",
    "\n",
    "    # CPU 실행\n",
    "    print(\"\\n▶ CPU 실행 중...\")\n",
    "    cpu_result = os.path.join(result_dir, \"cpu_result.csv\")\n",
    "    cpu_error = os.path.join(result_dir, \"cpu_error.txt\")\n",
    "    torch.cuda.set_device(-1)  # CPU 강제 지정 (optional, 실제 모델 내에서 device 고정해야 의미 있음)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    Paper_Contamination(\n",
    "        save_fullpath=cpu_result,\n",
    "        folder_path=paper_folder,\n",
    "        historical_data_fullpath=historical_data,\n",
    "        DB_fullpath=DB_path,\n",
    "        errorfile_fullpath=cpu_error\n",
    "    )\n",
    "\n",
    "    # 결과 비교\n",
    "    Paper_Contamination_Compare(gpu_result, cpu_result)\n",
    "\n",
    "# 실행\n",
    "run_contamination_dual()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc0c81-264a-4967-af32-909cf6d198cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import torch\n",
    "\n",
    "def compare_ocr_outputs(pdf_path, poppler_path=\"/usr/bin\"):\n",
    "    results = {\"cpu\": [], \"gpu\": []}\n",
    "\n",
    "    # PDF -> 이미지\n",
    "    pages = convert_from_path(pdf_path, dpi=400, poppler_path=poppler_path)\n",
    "\n",
    "    # 리더 생성\n",
    "    reader_cpu = easyocr.Reader(['en', 'ko'], gpu=False, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    reader_gpu = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        image_array = np.array(page)\n",
    "\n",
    "        # CPU OCR\n",
    "        try:\n",
    "            text_cpu = reader_cpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"cpu\"].append(\"\\n\".join(text_cpu))\n",
    "        except Exception as e:\n",
    "            results[\"cpu\"].append(f\"[CPU OCR 실패: {str(e)}]\")\n",
    "\n",
    "        # GPU OCR\n",
    "        try:\n",
    "            text_gpu = reader_gpu.readtext(image_array, detail=0, paragraph=1)\n",
    "            results[\"gpu\"].append(\"\\n\".join(text_gpu))\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                results[\"gpu\"].append(\"[GPU OCR 실패 - OOM]\")\n",
    "            else:\n",
    "                results[\"gpu\"].append(f\"[GPU OCR 실패: {str(e)}]\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()  # GPU 캐시 비우기\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_differences(cpu_result, gpu_result):\n",
    "    for i, (cpu, gpu) in enumerate(zip(cpu_result, gpu_result)):\n",
    "        print(f\"\\n📝 Page {i+1}\")\n",
    "        print(\"🔵 CPU:\")\n",
    "        print(cpu)\n",
    "        print(\"🟢 GPU:\")\n",
    "        print(gpu)\n",
    "\n",
    "# 🔽 여기에 PDF 경로만 넣으면 실행됨\n",
    "pdf_path = \"/home/taco/Documents/projects/jung/model/paper_folder/[고리4호기]증기발생기 'A' 수실 배수관의 배수밸브 용접부 누설부위 정비를 위한 원자로 수동정지.pdf\"\n",
    "result = compare_ocr_outputs(pdf_path)\n",
    "show_differences(result[\"cpu\"], result[\"gpu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03603d2c-61ed-418b-835c-623dfd166614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "import easyocr\n",
    "\n",
    "POPPLER_PATH = \"/usr/bin\"\n",
    "\n",
    "# 기존 OCR 함수 그대로 유지\n",
    "def paper_ocr(paper_name):\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=True, model_storage_directory='model', user_network_directory='model', download_enabled=False)\n",
    "    \n",
    "    if paper_name.endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            return [[f.read()]]  # 기존과 동일하게 리스트 형태로 반환\n",
    "\n",
    "    elif paper_name.endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, dpi=800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=False)  # 기존 코드와 일치\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "# 기존 OCR 함수를 사용하여 TXT 파일로 변환\n",
    "def convert_pdf_to_txt(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_names = [f for f in os.listdir(input_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in file_names:\n",
    "        pdf_path = os.path.join(input_folder, pdf_file)\n",
    "        txt_filename = pdf_file.replace('.pdf', '.txt')\n",
    "        txt_path = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "        # OCR 실행 (기존 paper_ocr과 동일한 방식)\n",
    "        try:\n",
    "            ocr_result = paper_ocr(pdf_path)\n",
    "            extracted_text = [\"\\n\".join(page) for page in ocr_result]\n",
    "\n",
    "            # 기존 문장 구조와 동일한 방식으로 저장\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                for page_num, text in enumerate(extracted_text, start=1):\n",
    "                    f.write(f\"[PAGE {page_num}]\\n{text}\\n\\n\")  # 기존 방식 유지\n",
    "\n",
    "            print(f\"✅ {pdf_file} → {txt_filename} 변환 완료\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {pdf_file} 변환 실패: {e}\")\n",
    "\n",
    "# 실행\n",
    "input_folder = \"paper_folder\"\n",
    "output_folder = os.path.join(input_folder, \"txt_output\")\n",
    "convert_pdf_to_txt(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b70540-0663-4e3f-9456-3e02698bd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "try:\n",
    "    llm = Llama(\n",
    "        model_path=GGUF_MODEL_PATH, \n",
    "        n_ctx=2048, \n",
    "        n_gpu_layers=0,  # GPU 미사용\n",
    "        n_batch=128,  # 배치 크기 줄이기\n",
    "        n_threads=4   # CPU 스레드 수 제한\n",
    "    )\n",
    "    print(\"✅ GGUF 모델 로드 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ GGUF 모델 로드 실패: {e}\")\n",
    "import os\n",
    "print(os.path.exists(GGUF_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a223fb4-3260-4805-a3dd-3d024882cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소\tNNG,*,F,형태소,Compound,*,*,형태/NNG/*+소/NNG/*\n",
      "분석\tNNG,행위,T,분석,*,*,*,*\n",
      "이\tJKS,*,F,이,*,*,*,*\n",
      "잘\tMAG,*,T,잘,*,*,*,*\n",
      "되\tVV,*,F,되,*,*,*,*\n",
      "나요\tEF,*,F,나요,*,*,*,*\n",
      "?\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "print(tagger.parse(\"형태소 분석이 잘 되나요?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f03578-2a7a-47ca-9eeb-e6b451893aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from llama_cpp import Llama  # GGUF 모델 실행을 위한 라이브러리\n",
    "import difflib\n",
    "\n",
    "# GGUF 모델 설정 (Mistral-7B GGUF 사용)\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q2_K_M.gguf\"\n",
    "\n",
    "# GGUF 모델 파일 존재 여부 확인\n",
    "if not os.path.exists(GGUF_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"GGUF 모델이 {GGUF_MODEL_PATH}에 존재하지 않습니다. 다운로드 후 다시 실행하세요.\")\n",
    "\n",
    "# GGUF 모델 로드 (CPU 실행)\n",
    "llm = Llama(model_path=GGUF_MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "# 결과 저장 경로\n",
    "save_fullpath = \"keyword_llm_gguf.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList_llm_gguf.txt\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    \"\"\"\n",
    "    PDF 또는 TXT 파일에서 텍스트를 추출하는 OCR 함수.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=False)\n",
    "\n",
    "    # .txt 파일 처리\n",
    "    if paper_name.lower().endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            paper_temp = [[f.read()]]\n",
    "        return paper_temp\n",
    "\n",
    "    # .pdf 파일 처리 (OCR)\n",
    "    elif paper_name.lower().endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, 800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "    return []\n",
    "\n",
    "def extract_contamination_with_llm(data, DB_room_df, DB_mc_df):\n",
    "    \"\"\"\n",
    "    GGUF 기반 LLM을 이용하여 격실 및 기기명 정보를 추출하고 DB와 매핑하는 함수.\n",
    "    \n",
    "    :param data: OCR에서 추출된 텍스트 데이터 (페이지별 리스트)\n",
    "    :param DB_room_df: 격실 DB\n",
    "    :param DB_mc_df: 기기 DB\n",
    "    :return: [{파일이름, 문장, 페이지, 문장위치, 격실, 기기명, 장소표현, 기기표현}...]\n",
    "    \"\"\"\n",
    "    extracted_results = []\n",
    "\n",
    "    # DB 데이터 로드\n",
    "    room_num = DB_room_df['격실번호'].astype(str).tolist()\n",
    "    room_en = DB_room_df['영문명'].astype(str).tolist()\n",
    "    room_ko = DB_room_df['한글명'].astype(str).tolist()\n",
    "    mc_num = DB_mc_df['기능위치'].astype(str).tolist()\n",
    "    mc_name = DB_mc_df['기능위치명'].astype(str).tolist()\n",
    "    mc_room = DB_mc_df['설치룸'].astype(str).tolist()\n",
    "\n",
    "    for page_idx, page_content in enumerate(data):\n",
    "        text_block = \" \".join(page_content)\n",
    "\n",
    "        # GGUF 모델을 이용한 LLM 실행\n",
    "        try:\n",
    "            response = llm(text_block, max_tokens=256)\n",
    "            if \"choices\" in response and isinstance(response[\"choices\"], list) and len(response[\"choices\"]) > 0:\n",
    "                extracted_text = response[\"choices\"][0][\"text\"]\n",
    "            else:\n",
    "                extracted_text = \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"LLM 처리 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "        # DB와 매칭 수행\n",
    "        matched_sentences = []\n",
    "        for sentence in extracted_text.split(\"\\n\"):\n",
    "            matched_row = {\"문장\": sentence.strip(), \"페이지\": page_idx + 1, \"문장위치\": len(matched_sentences) + 1, \"장소명\": \"\", \"기기명\": \"\", \"장소표현\": \"\", \"기기표현\": \"\"}\n",
    "\n",
    "            # 격실명 매칭\n",
    "            for word in room_ko + room_en + room_num:\n",
    "                if word in sentence:\n",
    "                    matched_row[\"장소명\"] = word\n",
    "                    matched_row[\"장소표현\"] = word\n",
    "\n",
    "            # 기기명 매칭\n",
    "            for word in mc_name + mc_num:\n",
    "                if word in sentence:\n",
    "                    matched_row[\"기기명\"] = word\n",
    "                    matched_row[\"기기표현\"] = word\n",
    "\n",
    "            matched_sentences.append(matched_row)\n",
    "\n",
    "        extracted_results.extend(matched_sentences)\n",
    "\n",
    "    return extracted_results\n",
    "\n",
    "# 실행 부분\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# DB 로드\n",
    "DB_room_df = pd.read_excel(DB_fullpath, sheet_name='격실 DB')\n",
    "DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='기기 DB')\n",
    "\n",
    "result_line_list = []\n",
    "errorfile = []\n",
    "\n",
    "for paper in file_names:\n",
    "    paper_name = os.path.join(folder_path, paper)\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "    except Exception as e:\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n",
    "\n",
    "    extracted_results = extract_contamination_with_llm(data, DB_room_df, DB_mc_df)\n",
    "\n",
    "    for row in extracted_results:\n",
    "        row[\"파일이름\"] = paper_name\n",
    "        result_line_list.append(row)\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame(result_line_list)\n",
    "result_df.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 오류 파일 목록 저장\n",
    "with open(errorfile_fullpath, \"w\") as file:\n",
    "    for item in errorfile:\n",
    "        file.write(str(item) + \"\\n\")\n",
    "\n",
    "print(\"GGUF 기반 LLM 격실 및 기기명 추출 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d96c38-f4a2-4165-969f-5317a1acd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# GGUF 모델 설정\n",
    "GGUF_MODEL_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\model\\mis\\mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# GGUF 모델 파일 확인\n",
    "if not os.path.exists(GGUF_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"GGUF 모델이 {GGUF_MODEL_PATH}에 없습니다. 다운로드 후 다시 실행하세요.\")\n",
    "\n",
    "# GGUF 모델 로드 (CPU 실행)\n",
    "llm = Llama(model_path=GGUF_MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "# PDF OCR 관련 설정\n",
    "POPPLER_PATH = r\"C:\\Users\\aidan\\Downloads\\ATC\\ATC\\Result\\Step 4\\poppler-23.01.0\\Library\\bin\"\n",
    "\n",
    "# 결과 저장 경로\n",
    "save_fullpath = \"keyword_llm_gguf.csv\"\n",
    "folder_path = \"paper_folder\"\n",
    "DB_fullpath = \"Model_DB.xlsx\"\n",
    "errorfile_fullpath = \"ErrorFileList_llm_gguf.txt\"\n",
    "\n",
    "def paper_ocr(paper_name):\n",
    "    \"\"\"PDF 또는 TXT 파일에서 텍스트를 추출하는 OCR 함수\"\"\"\n",
    "    reader = easyocr.Reader(['en', 'ko'], gpu=False)\n",
    "\n",
    "    if paper_name.lower().endswith('.txt'):\n",
    "        with open(paper_name, 'r', encoding='utf-8') as f:\n",
    "            return [[f.read()]]\n",
    "\n",
    "    elif paper_name.lower().endswith('.pdf'):\n",
    "        pages = convert_from_path(paper_name, 800, poppler_path=POPPLER_PATH)\n",
    "        paper_temp = []\n",
    "        for page in pages:\n",
    "            image_array = np.array(page)\n",
    "            result = reader.readtext(image_array, detail=0, paragraph=1)\n",
    "            paper_temp.append(result)\n",
    "        return paper_temp\n",
    "\n",
    "    return []\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"LLM이 반환한 텍스트에서 JSON 부분만 추출하는 함수\"\"\"\n",
    "    match = re.search(r'\\{[\\s\\S]*?\\}', text)  # JSON 패턴 찾기\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None  # JSON이 없으면 None 반환\n",
    "\n",
    "def extract_contamination_with_llm(data, DB_room_df, DB_mc_df):\n",
    "    \"\"\"\n",
    "    GGUF 기반 LLM을 이용하여 원자력 관련 격실 및 기기명 정보를 추출하고 DB와 매칭하는 함수.\n",
    "    \"\"\"\n",
    "    extracted_results = []\n",
    "\n",
    "    # DB에서 장소명과 기기명 리스트 생성 (결측치 제거 후 변환)\n",
    "    room_list = DB_room_df[\"장소명\"].dropna().astype(str).tolist()\n",
    "    mc_list = DB_mc_df[\"기기명\"].dropna().astype(str).tolist()\n",
    "\n",
    "    for page_idx, page_content in enumerate(data):\n",
    "        for sentence in page_content:  # 문장 단위로 LLM 실행\n",
    "            sentence = str(sentence).strip()  # 문장이 NaN이거나 float인 경우 대비\n",
    "\n",
    "            # 1. 문장에서 DB의 장소명/기기명을 찾기\n",
    "            matched_rooms = [room for room in room_list if room in sentence]\n",
    "            matched_mcs = [mc for mc in mc_list if mc in sentence]\n",
    "\n",
    "            # 2. 장소명/기기명이 없으면 LLM 호출하지 않음\n",
    "            if not matched_rooms and not matched_mcs:\n",
    "                continue\n",
    "\n",
    "            # 3. LLM 프롬프트에서 JSON 출력 강제\n",
    "            prompt = f\"\"\"\n",
    "            🔹 문장에서 장소명과 기기명을 JSON 형식으로 출력하세요.\n",
    "\n",
    "            **📌 주의사항**\n",
    "            - **JSON 외의 설명을 절대 포함하지 마세요.**\n",
    "            - 장소명은 '격실 DB'의 '한글명' 컬럼에서 가져온 값과 일치해야 합니다.\n",
    "            - 기기명은 '기기 DB'의 '기능위치명' 컬럼에서 가져온 값과 일치해야 합니다.\n",
    "\n",
    "            **📜 문장 예시**\n",
    "            \"{sentence}\"\n",
    "\n",
    "            **✅ JSON 형식 예제**\n",
    "            ```json\n",
    "            {{\n",
    "                \"장소명\": \"{matched_rooms[0] if matched_rooms else '미확인'}\",\n",
    "                \"기기명\": \"{matched_mcs[0] if matched_mcs else '미확인'}\",\n",
    "                \"장소표현\": \"원문에서 추출된 표현\",\n",
    "                \"기기표현\": \"원문에서 추출된 표현\"\n",
    "            }}\n",
    "            ```\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = llm.create_completion(\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=500,  # 충분한 응답 공간 확보\n",
    "                    stop=[\"\\n\\n\", \"```\"]  # 코드 블록 종료 강제\n",
    "                )\n",
    "                extracted_text = response[\"choices\"][0][\"text\"] if \"choices\" in response else \"\"\n",
    "\n",
    "                # JSON 추출 및 검증\n",
    "                json_text = extract_json_from_text(extracted_text)\n",
    "\n",
    "                if json_text:\n",
    "                    try:\n",
    "                        extracted_info = json.loads(json_text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"⚠️ JSON 변환 오류 발생. 원본 출력: {json_text}\")\n",
    "                        extracted_info = {}\n",
    "                else:\n",
    "                    print(\"⚠️ JSON 형식 감지 실패. LLM 응답 문제 가능성 있음.\")\n",
    "                    extracted_info = {}\n",
    "\n",
    "                # 결과 정리\n",
    "                matched_row = {\n",
    "                    \"문장\": sentence.strip(),\n",
    "                    \"페이지\": page_idx + 1,\n",
    "                    \"문장위치\": len(extracted_results) + 1,\n",
    "                    \"장소명\": extracted_info.get(\"장소명\", \"\"),\n",
    "                    \"기기명\": extracted_info.get(\"기기명\", \"\"),\n",
    "                    \"장소표현\": extracted_info.get(\"장소표현\", \"\"),\n",
    "                    \"기기표현\": extracted_info.get(\"기기표현\", \"\")\n",
    "                }\n",
    "                extracted_results.append(matched_row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ LLM 처리 오류: {e}\")\n",
    "                continue\n",
    "\n",
    "    return extracted_results\n",
    "\n",
    "# 실행 부분\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# DB 불러오기\n",
    "DB_room_df = pd.read_excel(DB_fullpath, sheet_name='격실 DB')\n",
    "DB_mc_df = pd.read_excel(DB_fullpath, sheet_name='기기 DB')\n",
    "\n",
    "# 컬럼명 자동 확인 후 매핑\n",
    "DB_room_df.rename(columns={\"한글명\": \"장소명\"}, inplace=True)\n",
    "DB_mc_df.rename(columns={\"기능위치명\": \"기기명\"}, inplace=True)\n",
    "\n",
    "result_line_list = []\n",
    "errorfile = []\n",
    "\n",
    "for paper in file_names:\n",
    "    paper_name = os.path.join(folder_path, paper)\n",
    "\n",
    "    try:\n",
    "        data = paper_ocr(paper_name)\n",
    "    except Exception as e:\n",
    "        errorfile.append(paper_name)\n",
    "        continue\n",
    "\n",
    "    extracted_results = extract_contamination_with_llm(data, DB_room_df, DB_mc_df)\n",
    "\n",
    "    for row in extracted_results:\n",
    "        row[\"파일이름\"] = paper_name\n",
    "        result_line_list.append(row)\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame(result_line_list)\n",
    "result_df.to_csv(save_fullpath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ GGUF 기반 LLM 격실 및 기기명 추출 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb12df-7bfc-4e4e-92cd-0762cee5991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)  # torch 버전 출력\n",
    "print(torchvision.__version__)  # torchvision 버전 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ba815-68b0-47bd-92df-aa8d98fc12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "sentences = [\"안녕하세요\", \"이 문장은 테스트를 위한 것입니다.\"]\n",
    "embeddings = embedder.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "from sentence_transformers import util\n",
    "\n",
    "# 두 문장의 유사도 계산\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5bde2-a45c-4ae1-87e0-5921cb2eb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "model = AutoModel.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# 테스트 문장\n",
    "text = \"한국어 문장을 테스트합니다.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 출력 텐서 확인\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2474c-b4c0-4e99-827f-1fca25598603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jung_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
